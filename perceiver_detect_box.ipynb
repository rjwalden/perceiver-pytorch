{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit (windows store)"
  },
  "interpreter": {
   "hash": "3677cddbc1f82f62e0ae465bd57603547b71c057823345d3a3ffe3c045472941"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  WARNING: The scripts wandb.exe and wb.exe are installed in 'C:\\Users\\Ryan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\Scripts' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\nWARNING: You are using pip version 21.1.1; however, version 21.1.2 is available.\nYou should consider upgrading via the 'C:\\Users\\Ryan\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb torch==1.8.1+cu111 -f https://download.pytorch.org/whl/torch_stable.html perceiver-pytorch opencv-python torchvision --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import wandb\n",
    "import torch\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from matplotlib import pyplot as plt\n",
    "from perceiver_pytorch import Perceiver\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?Perceiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Finishing last run (ID:141n7tbs) before initializing another..."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 23956<br/>Program ended successfully."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find user logs for this run at: <code>c:\\Users\\Ryan\\Repositories\\perceiver-pytorch\\wandb\\run-20210618_221758-141n7tbs\\logs\\debug.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find internal logs for this run at: <code>c:\\Users\\Ryan\\Repositories\\perceiver-pytorch\\wandb\\run-20210618_221758-141n7tbs\\logs\\debug-internal.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run summary:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>loss</td><td>0.00538</td></tr><tr><td>_runtime</td><td>2037</td></tr><tr><td>_timestamp</td><td>1624071115</td></tr><tr><td>_step</td><td>1249</td></tr></table>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run history:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>loss</td><td>█▆▅▃▃▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    <br/>Synced <strong style=\"color:#cdcd00\">legendary-wildflower-1</strong>: <a href=\"https://wandb.ai/rosenblatt/fnet-preceiver/runs/141n7tbs\" target=\"_blank\">https://wandb.ai/rosenblatt/fnet-preceiver/runs/141n7tbs</a><br/>\n                "
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "...Successfully finished last run (ID:141n7tbs). Initializing new run:<br/><br/>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.32<br/>\n                Syncing run <strong style=\"color:#cdcd00\">fast-bird-2</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/rosenblatt/fnet-preceiver\" target=\"_blank\">https://wandb.ai/rosenblatt/fnet-preceiver</a><br/>\n                Run page: <a href=\"https://wandb.ai/rosenblatt/fnet-preceiver/runs/mu0p5ldf\" target=\"_blank\">https://wandb.ai/rosenblatt/fnet-preceiver/runs/mu0p5ldf</a><br/>\n                Run data is saved locally in <code>c:\\Users\\Ryan\\Repositories\\perceiver-pytorch\\wandb\\run-20210618_225714-mu0p5ldf</code><br/><br/>\n            "
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/html": "<h1>Run(mu0p5ldf)</h1><iframe src=\"https://wandb.ai/rosenblatt/fnet-preceiver/runs/mu0p5ldf\" style=\"border:none;width:100%;height:400px\"></iframe>",
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1fbc6ee2160>"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "# 1. Start a W&B run\r\n",
    "wandb.init(project='fnet-preceiver', entity='rosenblatt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Save model inputs and hyperparameters\n",
    "config = wandb.config\n",
    "config.learning_rate = 1e-3\n",
    "config.batch_size = 128\n",
    "config.size = 64\n",
    "config.objects = 1\n",
    "config.epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Perceiver(\n",
    "    input_channels = 3,          # number of channels for each token of the input\n",
    "    input_axis = 2,              # number of axis for input data (2 for images, 3 for video)\n",
    "    num_freq_bands = 4,          # number of freq bands, with original value (2 * K + 1)\n",
    "    max_freq = 5.,              # maximum frequency, hyperparameter depending on how fine the data is\n",
    "    depth = 2,                   # depth of net\n",
    "    num_latents = 32,           # number of latents, or induced set points, or centroids. different papers giving it different names\n",
    "    # cross_dim = 32,             # cross attention dimension\n",
    "    latent_dim = 32,            # latent dimension\n",
    "    cross_heads = 1,             # number of heads for cross attention. paper said 1\n",
    "    latent_heads = 4,            # number of heads for latent self attention, 8\n",
    "    cross_dim_head = 32,\n",
    "    latent_dim_head = 32,\n",
    "    num_classes = 4*config.objects,          # output number of classes\n",
    "    attn_dropout = 0.5,\n",
    "    ff_dropout = 0.5,\n",
    "    weight_tie_layers = True    # whether to weight tie layers (optional, as indicated in the diagram)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model on very easy object detection problem\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self):   \n",
    "      pass\n",
    "      \n",
    "    def __len__(self):\n",
    "        return 320000\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = np.zeros((config.size,config.size,3), np.uint8)\n",
    "        labels = []\n",
    "\n",
    "        for i in range(config.objects):\n",
    "          if (np.random.rand() > 0.02):\n",
    "            point_x = int(np.random.rand() * size)\n",
    "            point_y = int(np.random.rand() * size)\n",
    "            size_x = int(np.random.rand() * size)\n",
    "            size_y = int(np.random.rand() * size)       \n",
    "            r, g, b = int(np.random.rand()*255), int(np.random.rand()*255), int(np.random.rand() * 255)  \n",
    "            try:\n",
    "              cv2.rectangle(image, (int(point_x - size_x/2), int(point_y - size_y/2)), (int(point_x + size_x/2), int(point_y + size_y/2)), (r, g, b), -1)            \n",
    "              labels.append((point_x/size, point_y/size, size_x/size, size_y/size))\n",
    "            except Exception as e:\n",
    "              print(e)\n",
    "              labels.append((0,0,0,0))\n",
    "          else:            \n",
    "            labels.append((0,0,0,0))            \n",
    "                  \n",
    "        labels = torch.as_tensor(labels, dtype=torch.float32)\n",
    "        image = torch.as_tensor(image, dtype=torch.float32)\n",
    "\n",
    "        return (image, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Perceiver(\n",
       "  (layers): ModuleList(\n",
       "    (0): ModuleList(\n",
       "      (0): PreNorm(\n",
       "        (fn): Attention(\n",
       "          (to_q): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (to_kv): Linear(in_features=21, out_features=64, bias=False)\n",
       "          (to_out): Sequential(\n",
       "            (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (1): Dropout(p=0.5, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm_context): LayerNorm((21,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): PreNorm(\n",
       "        (fn): FeedForward(\n",
       "          (net): Sequential(\n",
       "            (0): Linear(in_features=32, out_features=256, bias=True)\n",
       "            (1): GEGLU()\n",
       "            (2): Dropout(p=0.5, inplace=False)\n",
       "            (3): Linear(in_features=128, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): PreNorm(\n",
       "            (fn): Attention(\n",
       "              (to_q): Linear(in_features=32, out_features=128, bias=False)\n",
       "              (to_kv): Linear(in_features=32, out_features=256, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=128, out_features=32, bias=True)\n",
       "                (1): Dropout(p=0.5, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): PreNorm(\n",
       "            (fn): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Linear(in_features=32, out_features=256, bias=True)\n",
       "                (1): GEGLU()\n",
       "                (2): Dropout(p=0.5, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=32, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): ModuleList(\n",
       "      (0): PreNorm(\n",
       "        (fn): Attention(\n",
       "          (to_q): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (to_kv): Linear(in_features=21, out_features=64, bias=False)\n",
       "          (to_out): Sequential(\n",
       "            (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (1): Dropout(p=0.5, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm_context): LayerNorm((21,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): PreNorm(\n",
       "        (fn): FeedForward(\n",
       "          (net): Sequential(\n",
       "            (0): Linear(in_features=32, out_features=256, bias=True)\n",
       "            (1): GEGLU()\n",
       "            (2): Dropout(p=0.5, inplace=False)\n",
       "            (3): Linear(in_features=128, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): PreNorm(\n",
       "            (fn): Attention(\n",
       "              (to_q): Linear(in_features=32, out_features=128, bias=False)\n",
       "              (to_kv): Linear(in_features=32, out_features=256, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=128, out_features=32, bias=True)\n",
       "                (1): Dropout(p=0.5, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): PreNorm(\n",
       "            (fn): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Linear(in_features=32, out_features=256, bias=True)\n",
       "                (1): GEGLU()\n",
       "                (2): Dropout(p=0.5, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=32, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (to_logits): Sequential(\n",
       "    (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=32, out_features=4, bias=True)\n",
       "  )\n",
       "  (sinu_emb): SinusoidalEmbeddings()\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "criterion = torch.nn.MSELoss().cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=config.learning_rate, amsgrad=True)\n",
    "\n",
    "dataset = CustomDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "wandb.watch(model)\n",
    "model.cuda()\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "05\n",
      "[5,    300] loss: 0.006552\n",
      "[5,    320] loss: 0.006522\n",
      "[5,    340] loss: 0.006986\n",
      "[5,    360] loss: 0.007159\n",
      "[5,    380] loss: 0.007302\n",
      "[5,    400] loss: 0.007393\n",
      "[5,    420] loss: 0.006705\n",
      "[5,    440] loss: 0.006233\n",
      "[5,    460] loss: 0.006538\n",
      "[5,    480] loss: 0.006422\n",
      "[5,    500] loss: 0.006837\n",
      "[5,    520] loss: 0.006619\n",
      "[5,    540] loss: 0.007128\n",
      "[5,    560] loss: 0.007018\n",
      "[5,    580] loss: 0.006646\n",
      "[5,    600] loss: 0.006866\n",
      "[5,    620] loss: 0.006590\n",
      "[5,    640] loss: 0.006393\n",
      "[5,    660] loss: 0.006458\n",
      "[5,    680] loss: 0.006340\n",
      "[5,    700] loss: 0.006576\n",
      "[5,    720] loss: 0.006785\n",
      "[5,    740] loss: 0.006563\n",
      "[5,    760] loss: 0.006867\n",
      "[5,    780] loss: 0.006315\n",
      "[5,    800] loss: 0.006331\n",
      "[5,    820] loss: 0.006823\n",
      "[5,    840] loss: 0.006597\n",
      "[5,    860] loss: 0.006772\n",
      "[5,    880] loss: 0.006601\n",
      "[5,    900] loss: 0.006149\n",
      "[5,    920] loss: 0.006064\n",
      "[5,    940] loss: 0.006292\n",
      "[5,    960] loss: 0.006178\n",
      "[5,    980] loss: 0.006522\n",
      "[5,   1000] loss: 0.006715\n",
      "[5,   1020] loss: 0.006490\n",
      "[5,   1040] loss: 0.006686\n",
      "[5,   1060] loss: 0.006459\n",
      "[5,   1080] loss: 0.005840\n",
      "[5,   1100] loss: 0.006963\n",
      "[5,   1120] loss: 0.006769\n",
      "[5,   1140] loss: 0.006387\n",
      "[5,   1160] loss: 0.006846\n",
      "[5,   1180] loss: 0.006712\n",
      "[5,   1200] loss: 0.006065\n",
      "[5,   1220] loss: 0.006583\n",
      "[5,   1240] loss: 0.006471\n",
      "[5,   1260] loss: 0.006244\n",
      "[5,   1280] loss: 0.006347\n",
      "[5,   1300] loss: 0.005954\n",
      "[5,   1320] loss: 0.005929\n",
      "[5,   1340] loss: 0.006204\n",
      "[5,   1360] loss: 0.006656\n",
      "[5,   1380] loss: 0.006123\n",
      "[5,   1400] loss: 0.006240\n",
      "[5,   1420] loss: 0.006562\n",
      "[5,   1440] loss: 0.006163\n",
      "[5,   1460] loss: 0.006174\n",
      "[5,   1480] loss: 0.006365\n",
      "[5,   1500] loss: 0.006767\n",
      "[5,   1520] loss: 0.006181\n",
      "[5,   1540] loss: 0.006765\n",
      "[5,   1560] loss: 0.006300\n",
      "[5,   1580] loss: 0.006366\n",
      "[5,   1600] loss: 0.006178\n",
      "[5,   1620] loss: 0.006279\n",
      "[5,   1640] loss: 0.006220\n",
      "[5,   1660] loss: 0.006572\n",
      "[5,   1680] loss: 0.006326\n",
      "[5,   1700] loss: 0.006833\n",
      "[5,   1720] loss: 0.007228\n",
      "[5,   1740] loss: 0.006437\n",
      "[5,   1760] loss: 0.006482\n",
      "[5,   1780] loss: 0.006073\n",
      "[5,   1800] loss: 0.006698\n",
      "[5,   1820] loss: 0.006608\n",
      "[5,   1840] loss: 0.006543\n",
      "[5,   1860] loss: 0.006938\n",
      "[5,   1880] loss: 0.006489\n",
      "[5,   1900] loss: 0.006097\n",
      "[5,   1920] loss: 0.005940\n",
      "[5,   1940] loss: 0.006437\n",
      "[5,   1960] loss: 0.006153\n",
      "[5,   1980] loss: 0.006748\n",
      "[5,   2000] loss: 0.006463\n",
      "[5,   2020] loss: 0.006172\n",
      "[5,   2040] loss: 0.006278\n",
      "[5,   2060] loss: 0.006249\n",
      "[5,   2080] loss: 0.006455\n",
      "[5,   2100] loss: 0.006300\n",
      "[5,   2120] loss: 0.006313\n",
      "[5,   2140] loss: 0.006362\n",
      "[5,   2160] loss: 0.006539\n",
      "[5,   2180] loss: 0.006316\n",
      "[5,   2200] loss: 0.006966\n",
      "[5,   2220] loss: 0.006487\n",
      "[5,   2240] loss: 0.006313\n",
      "[5,   2260] loss: 0.006378\n",
      "[5,   2280] loss: 0.006176\n",
      "[5,   2300] loss: 0.006591\n",
      "[5,   2320] loss: 0.006227\n",
      "[5,   2340] loss: 0.006622\n",
      "[5,   2360] loss: 0.006118\n",
      "[5,   2380] loss: 0.006867\n",
      "[5,   2400] loss: 0.006556\n",
      "[5,   2420] loss: 0.006732\n",
      "[5,   2440] loss: 0.006741\n",
      "[5,   2460] loss: 0.006316\n",
      "[5,   2480] loss: 0.005851\n",
      "[5,   2500] loss: 0.006332\n",
      "[6,     20] loss: 0.006174\n",
      "[6,     40] loss: 0.006304\n",
      "[6,     60] loss: 0.006181\n",
      "[6,     80] loss: 0.006251\n",
      "[6,    100] loss: 0.006244\n",
      "[6,    120] loss: 0.005805\n",
      "[6,    140] loss: 0.006039\n",
      "[6,    160] loss: 0.006256\n",
      "[6,    180] loss: 0.006065\n",
      "[6,    200] loss: 0.006822\n",
      "[6,    220] loss: 0.006108\n",
      "[6,    240] loss: 0.006639\n",
      "[6,    260] loss: 0.006510\n",
      "[6,    280] loss: 0.006520\n",
      "[6,    300] loss: 0.006260\n",
      "[6,    320] loss: 0.006096\n",
      "[6,    340] loss: 0.006171\n",
      "[6,    360] loss: 0.006244\n",
      "[6,    380] loss: 0.006379\n",
      "[6,    400] loss: 0.005858\n",
      "[6,    420] loss: 0.005964\n",
      "[6,    440] loss: 0.006101\n",
      "[6,    460] loss: 0.006027\n",
      "[6,    480] loss: 0.006241\n",
      "[6,    500] loss: 0.006335\n",
      "[6,    520] loss: 0.006602\n",
      "[6,    540] loss: 0.006812\n",
      "[6,    560] loss: 0.006591\n",
      "[6,    580] loss: 0.006465\n",
      "[6,    600] loss: 0.006219\n",
      "[6,    620] loss: 0.006426\n",
      "[6,    640] loss: 0.006049\n",
      "[6,    660] loss: 0.006189\n",
      "[6,    680] loss: 0.005996\n",
      "[6,    700] loss: 0.006392\n",
      "[6,    720] loss: 0.006332\n",
      "[6,    740] loss: 0.006503\n",
      "[6,    760] loss: 0.005944\n",
      "[6,    780] loss: 0.006319\n",
      "[6,    800] loss: 0.006488\n",
      "[6,    820] loss: 0.005846\n",
      "[6,    840] loss: 0.006219\n",
      "[6,    860] loss: 0.005946\n",
      "[6,    880] loss: 0.005924\n",
      "[6,    900] loss: 0.006372\n",
      "[6,    920] loss: 0.006606\n",
      "[6,    940] loss: 0.006155\n",
      "[6,    960] loss: 0.006494\n",
      "[6,    980] loss: 0.006299\n",
      "[6,   1000] loss: 0.006236\n",
      "[6,   1020] loss: 0.006278\n",
      "[6,   1040] loss: 0.006119\n",
      "[6,   1060] loss: 0.006466\n",
      "[6,   1080] loss: 0.006035\n",
      "[6,   1100] loss: 0.006542\n",
      "[6,   1120] loss: 0.006589\n",
      "[6,   1140] loss: 0.006805\n",
      "[6,   1160] loss: 0.006062\n",
      "[6,   1180] loss: 0.006577\n",
      "[6,   1200] loss: 0.006484\n",
      "[6,   1220] loss: 0.006044\n",
      "[6,   1240] loss: 0.005906\n",
      "[6,   1260] loss: 0.006417\n",
      "[6,   1280] loss: 0.006640\n",
      "[6,   1300] loss: 0.006075\n",
      "[6,   1320] loss: 0.006204\n",
      "[6,   1340] loss: 0.006181\n",
      "[6,   1360] loss: 0.006784\n",
      "[6,   1380] loss: 0.006201\n",
      "[6,   1400] loss: 0.005843\n",
      "[6,   1420] loss: 0.005865\n",
      "[6,   1440] loss: 0.005710\n",
      "[6,   1460] loss: 0.006292\n",
      "[6,   1480] loss: 0.006565\n",
      "[6,   1500] loss: 0.006243\n",
      "[6,   1520] loss: 0.006103\n",
      "[6,   1540] loss: 0.005886\n",
      "[6,   1560] loss: 0.005721\n",
      "[6,   1580] loss: 0.006002\n",
      "[6,   1600] loss: 0.006207\n",
      "[6,   1620] loss: 0.006151\n",
      "[6,   1640] loss: 0.006188\n",
      "[6,   1660] loss: 0.005906\n",
      "[6,   1680] loss: 0.006423\n",
      "[6,   1700] loss: 0.005907\n",
      "[6,   1720] loss: 0.005905\n",
      "[6,   1740] loss: 0.006041\n",
      "[6,   1760] loss: 0.006149\n",
      "[6,   1780] loss: 0.005990\n",
      "[6,   1800] loss: 0.005976\n",
      "[6,   1820] loss: 0.006489\n",
      "[6,   1840] loss: 0.006156\n",
      "[6,   1860] loss: 0.005869\n",
      "[6,   1880] loss: 0.006029\n",
      "[6,   1900] loss: 0.006275\n",
      "[6,   1920] loss: 0.006183\n",
      "[6,   1940] loss: 0.006241\n",
      "[6,   1960] loss: 0.006437\n",
      "[6,   1980] loss: 0.006642\n",
      "[6,   2000] loss: 0.006091\n",
      "[6,   2020] loss: 0.006000\n",
      "[6,   2040] loss: 0.006729\n",
      "[6,   2060] loss: 0.006053\n",
      "[6,   2080] loss: 0.006066\n",
      "[6,   2100] loss: 0.006058\n",
      "[6,   2120] loss: 0.005863\n",
      "[6,   2140] loss: 0.006364\n",
      "[6,   2160] loss: 0.006767\n",
      "[6,   2180] loss: 0.005999\n",
      "[6,   2200] loss: 0.005771\n",
      "[6,   2220] loss: 0.005979\n",
      "[6,   2240] loss: 0.006353\n",
      "[6,   2260] loss: 0.005685\n",
      "[6,   2280] loss: 0.006069\n",
      "[6,   2300] loss: 0.006096\n",
      "[6,   2320] loss: 0.005860\n",
      "[6,   2340] loss: 0.005963\n",
      "[6,   2360] loss: 0.005962\n",
      "[6,   2380] loss: 0.005901\n",
      "[6,   2400] loss: 0.006406\n",
      "[6,   2420] loss: 0.006097\n",
      "[6,   2440] loss: 0.006374\n",
      "[6,   2460] loss: 0.006334\n",
      "[6,   2480] loss: 0.006055\n",
      "[6,   2500] loss: 0.005672\n",
      "[7,     20] loss: 0.005722\n",
      "[7,     40] loss: 0.005996\n",
      "[7,     60] loss: 0.005911\n",
      "[7,     80] loss: 0.005922\n",
      "[7,    100] loss: 0.005759\n",
      "[7,    120] loss: 0.005972\n",
      "[7,    140] loss: 0.006244\n",
      "[7,    160] loss: 0.006039\n",
      "[7,    180] loss: 0.006076\n",
      "[7,    200] loss: 0.006030\n",
      "[7,    220] loss: 0.005835\n",
      "[7,    240] loss: 0.006091\n",
      "[7,    260] loss: 0.007067\n",
      "[7,    280] loss: 0.006211\n",
      "[7,    300] loss: 0.005811\n",
      "[7,    320] loss: 0.006083\n",
      "[7,    340] loss: 0.006105\n",
      "[7,    360] loss: 0.005947\n",
      "[7,    380] loss: 0.005879\n",
      "[7,    400] loss: 0.005775\n",
      "[7,    420] loss: 0.005500\n",
      "[7,    440] loss: 0.006167\n",
      "[7,    460] loss: 0.005773\n",
      "[7,    480] loss: 0.006014\n",
      "[7,    500] loss: 0.005990\n",
      "[7,    520] loss: 0.006059\n",
      "[7,    540] loss: 0.005891\n",
      "[7,    560] loss: 0.006005\n",
      "[7,    580] loss: 0.005787\n",
      "[7,    600] loss: 0.005970\n",
      "[7,    620] loss: 0.006099\n",
      "[7,    640] loss: 0.005998\n",
      "[7,    660] loss: 0.006025\n",
      "[7,    680] loss: 0.006085\n",
      "[7,    700] loss: 0.006142\n",
      "[7,    720] loss: 0.006009\n",
      "[7,    740] loss: 0.005692\n",
      "[7,    760] loss: 0.005888\n",
      "[7,    780] loss: 0.006101\n",
      "[7,    800] loss: 0.006183\n",
      "[7,    820] loss: 0.005855\n",
      "[7,    840] loss: 0.005795\n",
      "[7,    860] loss: 0.005970\n",
      "[7,    880] loss: 0.006533\n",
      "[7,    900] loss: 0.005822\n",
      "[7,    920] loss: 0.005898\n",
      "[7,    940] loss: 0.006612\n",
      "[7,    960] loss: 0.005777\n",
      "[7,    980] loss: 0.005900\n",
      "[7,   1000] loss: 0.005677\n",
      "[7,   1020] loss: 0.005843\n",
      "[7,   1040] loss: 0.006139\n",
      "[7,   1060] loss: 0.005935\n",
      "[7,   1080] loss: 0.005791\n",
      "[7,   1100] loss: 0.005804\n",
      "[7,   1120] loss: 0.005691\n",
      "[7,   1140] loss: 0.006291\n",
      "[7,   1160] loss: 0.005650\n",
      "[7,   1180] loss: 0.005586\n",
      "[7,   1200] loss: 0.005656\n",
      "[7,   1220] loss: 0.006053\n",
      "[7,   1240] loss: 0.006046\n",
      "[7,   1260] loss: 0.006216\n",
      "[7,   1280] loss: 0.006430\n",
      "[7,   1300] loss: 0.005954\n",
      "[7,   1320] loss: 0.005943\n",
      "[7,   1340] loss: 0.006072\n",
      "[7,   1360] loss: 0.006324\n",
      "[7,   1380] loss: 0.005756\n",
      "[7,   1400] loss: 0.005612\n",
      "[7,   1420] loss: 0.005643\n",
      "[7,   1440] loss: 0.005909\n",
      "[7,   1460] loss: 0.006099\n",
      "[7,   1480] loss: 0.006067\n",
      "[7,   1500] loss: 0.005908\n",
      "[7,   1520] loss: 0.005710\n",
      "[7,   1540] loss: 0.005786\n",
      "[7,   1560] loss: 0.005868\n",
      "[7,   1580] loss: 0.005714\n",
      "[7,   1600] loss: 0.005939\n",
      "[7,   1620] loss: 0.005645\n",
      "[7,   1640] loss: 0.005794\n",
      "[7,   1660] loss: 0.006231\n",
      "[7,   1680] loss: 0.005787\n",
      "[7,   1700] loss: 0.005650\n",
      "[7,   1720] loss: 0.005725\n",
      "[7,   1740] loss: 0.005853\n",
      "[7,   1760] loss: 0.005806\n",
      "[7,   1780] loss: 0.005636\n",
      "[7,   1800] loss: 0.005778\n",
      "[7,   1820] loss: 0.006107\n",
      "[7,   1840] loss: 0.006226\n",
      "[7,   1860] loss: 0.006127\n",
      "[7,   1880] loss: 0.005942\n",
      "[7,   1900] loss: 0.005920\n",
      "[7,   1920] loss: 0.005914\n",
      "[7,   1940] loss: 0.006012\n",
      "[7,   1960] loss: 0.005867\n",
      "[7,   1980] loss: 0.006017\n",
      "[7,   2000] loss: 0.005851\n",
      "[7,   2020] loss: 0.005619\n",
      "[7,   2040] loss: 0.005689\n",
      "[7,   2060] loss: 0.005715\n",
      "[7,   2080] loss: 0.005754\n",
      "[7,   2100] loss: 0.005801\n",
      "[7,   2120] loss: 0.005728\n",
      "[7,   2140] loss: 0.005772\n",
      "[7,   2160] loss: 0.005594\n",
      "[7,   2180] loss: 0.005633\n",
      "[7,   2200] loss: 0.006067\n",
      "[7,   2220] loss: 0.005845\n",
      "[7,   2240] loss: 0.005939\n",
      "[7,   2260] loss: 0.005732\n",
      "[7,   2280] loss: 0.005803\n",
      "[7,   2300] loss: 0.005660\n",
      "[7,   2320] loss: 0.005839\n",
      "[7,   2340] loss: 0.006006\n",
      "[7,   2360] loss: 0.006001\n",
      "[7,   2380] loss: 0.005739\n",
      "[7,   2400] loss: 0.006055\n",
      "[7,   2420] loss: 0.005911\n",
      "[7,   2440] loss: 0.006097\n",
      "[7,   2460] loss: 0.005726\n",
      "[7,   2480] loss: 0.006027\n",
      "[7,   2500] loss: 0.005790\n",
      "[8,     20] loss: 0.005868\n",
      "[8,     40] loss: 0.006080\n",
      "[8,     60] loss: 0.006046\n",
      "[8,     80] loss: 0.005606\n",
      "[8,    100] loss: 0.005954\n",
      "[8,    120] loss: 0.005695\n",
      "[8,    140] loss: 0.006997\n",
      "[8,    160] loss: 0.007393\n",
      "[8,    180] loss: 0.005725\n",
      "[8,    200] loss: 0.005797\n",
      "[8,    220] loss: 0.005778\n",
      "[8,    240] loss: 0.005572\n",
      "[8,    260] loss: 0.005857\n",
      "[8,    280] loss: 0.005760\n",
      "[8,    300] loss: 0.005688\n",
      "[8,    320] loss: 0.005733\n",
      "[8,    340] loss: 0.005946\n",
      "[8,    360] loss: 0.005621\n",
      "[8,    380] loss: 0.005616\n",
      "[8,    400] loss: 0.005858\n",
      "[8,    420] loss: 0.005454\n",
      "[8,    440] loss: 0.005596\n",
      "[8,    460] loss: 0.005566\n",
      "[8,    480] loss: 0.005729\n",
      "[8,    500] loss: 0.005605\n",
      "[8,    520] loss: 0.005832\n",
      "[8,    540] loss: 0.005776\n",
      "[8,    560] loss: 0.006483\n",
      "[8,    580] loss: 0.006159\n",
      "[8,    600] loss: 0.005350\n",
      "[8,    620] loss: 0.006052\n",
      "[8,    640] loss: 0.005697\n",
      "[8,    660] loss: 0.005322\n",
      "[8,    680] loss: 0.005534\n",
      "[8,    700] loss: 0.005837\n",
      "[8,    720] loss: 0.005666\n",
      "[8,    740] loss: 0.005997\n",
      "[8,    760] loss: 0.005886\n",
      "[8,    780] loss: 0.006067\n",
      "[8,    800] loss: 0.006014\n",
      "[8,    820] loss: 0.005884\n",
      "[8,    840] loss: 0.005327\n",
      "[8,    860] loss: 0.005506\n",
      "[8,    880] loss: 0.005825\n",
      "[8,    900] loss: 0.005942\n",
      "[8,    920] loss: 0.006332\n",
      "[8,    940] loss: 0.005662\n",
      "[8,    960] loss: 0.005663\n",
      "[8,    980] loss: 0.005609\n",
      "[8,   1000] loss: 0.005769\n",
      "[8,   1020] loss: 0.005930\n",
      "[8,   1040] loss: 0.005593\n",
      "[8,   1060] loss: 0.006157\n",
      "[8,   1080] loss: 0.005801\n",
      "[8,   1100] loss: 0.005647\n",
      "[8,   1120] loss: 0.005914\n",
      "[8,   1140] loss: 0.005434\n",
      "[8,   1160] loss: 0.005822\n",
      "[8,   1180] loss: 0.005593\n",
      "[8,   1200] loss: 0.005759\n",
      "[8,   1220] loss: 0.005605\n",
      "[8,   1240] loss: 0.005791\n",
      "[8,   1260] loss: 0.005705\n",
      "[8,   1280] loss: 0.005601\n",
      "[8,   1300] loss: 0.005484\n",
      "[8,   1320] loss: 0.006028\n",
      "[8,   1340] loss: 0.005774\n",
      "[8,   1360] loss: 0.005688\n",
      "[8,   1380] loss: 0.005690\n",
      "[8,   1400] loss: 0.005652\n",
      "[8,   1420] loss: 0.005906\n",
      "[8,   1440] loss: 0.005654\n",
      "[8,   1460] loss: 0.006083\n",
      "[8,   1480] loss: 0.005881\n",
      "[8,   1500] loss: 0.005850\n",
      "[8,   1520] loss: 0.005776\n",
      "[8,   1540] loss: 0.005374\n",
      "[8,   1560] loss: 0.005823\n",
      "[8,   1580] loss: 0.006010\n",
      "[8,   1600] loss: 0.005801\n",
      "[8,   1620] loss: 0.005569\n",
      "[8,   1640] loss: 0.005731\n",
      "[8,   1660] loss: 0.005551\n",
      "[8,   1680] loss: 0.005806\n",
      "[8,   1700] loss: 0.005838\n",
      "[8,   1720] loss: 0.005822\n",
      "[8,   1740] loss: 0.005755\n",
      "[8,   1760] loss: 0.005413\n",
      "[8,   1780] loss: 0.005245\n",
      "[8,   1800] loss: 0.005735\n",
      "[8,   1820] loss: 0.005848\n",
      "[8,   1840] loss: 0.005861\n",
      "[8,   1860] loss: 0.006002\n",
      "[8,   1880] loss: 0.005737\n",
      "[8,   1900] loss: 0.005667\n",
      "[8,   1920] loss: 0.006099\n",
      "[8,   1940] loss: 0.006672\n",
      "[8,   1960] loss: 0.005760\n",
      "[8,   1980] loss: 0.005489\n",
      "[8,   2000] loss: 0.005475\n",
      "[8,   2020] loss: 0.005811\n",
      "[8,   2040] loss: 0.005434\n",
      "[8,   2060] loss: 0.006015\n",
      "[8,   2080] loss: 0.005607\n",
      "[8,   2100] loss: 0.005832\n",
      "[8,   2120] loss: 0.005890\n",
      "[8,   2140] loss: 0.005747\n",
      "[8,   2160] loss: 0.005404\n",
      "[8,   2180] loss: 0.005584\n",
      "[8,   2200] loss: 0.005335\n",
      "[8,   2220] loss: 0.005508\n",
      "[8,   2240] loss: 0.005806\n",
      "[8,   2260] loss: 0.005898\n",
      "[8,   2280] loss: 0.005596\n",
      "[8,   2300] loss: 0.005658\n",
      "[8,   2320] loss: 0.005530\n",
      "[8,   2340] loss: 0.005496\n",
      "[8,   2360] loss: 0.006319\n",
      "[8,   2380] loss: 0.005264\n",
      "[8,   2400] loss: 0.005734\n",
      "[8,   2420] loss: 0.005527\n",
      "[8,   2440] loss: 0.005672\n",
      "[8,   2460] loss: 0.005737\n",
      "[8,   2480] loss: 0.005400\n",
      "[8,   2500] loss: 0.005252\n",
      "[9,     20] loss: 0.005247\n",
      "[9,     40] loss: 0.005531\n",
      "[9,     60] loss: 0.005610\n",
      "[9,     80] loss: 0.005540\n",
      "[9,    100] loss: 0.005202\n",
      "[9,    120] loss: 0.005623\n",
      "[9,    140] loss: 0.005362\n",
      "[9,    160] loss: 0.005393\n",
      "[9,    180] loss: 0.005628\n",
      "[9,    200] loss: 0.005781\n",
      "[9,    220] loss: 0.005577\n",
      "[9,    240] loss: 0.005377\n",
      "[9,    260] loss: 0.005661\n",
      "[9,    280] loss: 0.006032\n",
      "[9,    300] loss: 0.005761\n",
      "[9,    320] loss: 0.006054\n",
      "[9,    340] loss: 0.005979\n",
      "[9,    360] loss: 0.005775\n",
      "[9,    380] loss: 0.005436\n",
      "[9,    400] loss: 0.005482\n",
      "[9,    420] loss: 0.006007\n",
      "[9,    440] loss: 0.006029\n",
      "[9,    460] loss: 0.005445\n",
      "[9,    480] loss: 0.005480\n",
      "[9,    500] loss: 0.005537\n",
      "[9,    520] loss: 0.005563\n",
      "[9,    540] loss: 0.005404\n",
      "[9,    560] loss: 0.005448\n",
      "[9,    580] loss: 0.005911\n",
      "[9,    600] loss: 0.006688\n",
      "[9,    620] loss: 0.006286\n",
      "[9,    640] loss: 0.005438\n",
      "[9,    660] loss: 0.005330\n",
      "[9,    680] loss: 0.005435\n",
      "[9,    700] loss: 0.005484\n",
      "[9,    720] loss: 0.005556\n",
      "[9,    740] loss: 0.006032\n",
      "[9,    760] loss: 0.005599\n",
      "[9,    780] loss: 0.005861\n",
      "[9,    800] loss: 0.005742\n",
      "[9,    820] loss: 0.005699\n",
      "[9,    840] loss: 0.005635\n",
      "[9,    860] loss: 0.005892\n",
      "[9,    880] loss: 0.005736\n",
      "[9,    900] loss: 0.005772\n",
      "[9,    920] loss: 0.005418\n",
      "[9,    940] loss: 0.005319\n",
      "[9,    960] loss: 0.005610\n",
      "[9,    980] loss: 0.005743\n",
      "[9,   1000] loss: 0.005733\n",
      "[9,   1020] loss: 0.005800\n",
      "[9,   1040] loss: 0.005382\n",
      "[9,   1060] loss: 0.005476\n",
      "[9,   1080] loss: 0.005674\n",
      "[9,   1100] loss: 0.005643\n",
      "[9,   1120] loss: 0.005725\n",
      "[9,   1140] loss: 0.005621\n",
      "[9,   1160] loss: 0.006111\n",
      "[9,   1180] loss: 0.005411\n",
      "[9,   1200] loss: 0.006044\n",
      "[9,   1220] loss: 0.005694\n",
      "[9,   1240] loss: 0.006492\n",
      "[9,   1260] loss: 0.005683\n",
      "[9,   1280] loss: 0.005587\n",
      "[9,   1300] loss: 0.005797\n",
      "[9,   1320] loss: 0.005622\n",
      "[9,   1340] loss: 0.005436\n",
      "[9,   1360] loss: 0.005288\n",
      "[9,   1380] loss: 0.005628\n",
      "[9,   1400] loss: 0.005696\n",
      "[9,   1420] loss: 0.005310\n",
      "[9,   1440] loss: 0.005235\n",
      "[9,   1460] loss: 0.005270\n",
      "[9,   1480] loss: 0.005377\n",
      "[9,   1500] loss: 0.005344\n",
      "[9,   1520] loss: 0.005556\n",
      "[9,   1540] loss: 0.005338\n",
      "[9,   1560] loss: 0.005258\n",
      "[9,   1580] loss: 0.005403\n",
      "[9,   1600] loss: 0.005768\n",
      "[9,   1620] loss: 0.005506\n",
      "[9,   1640] loss: 0.005634\n",
      "[9,   1660] loss: 0.006120\n",
      "[9,   1680] loss: 0.005718\n",
      "[9,   1700] loss: 0.006137\n",
      "[9,   1720] loss: 0.005363\n",
      "[9,   1740] loss: 0.005528\n",
      "[9,   1760] loss: 0.005499\n",
      "[9,   1780] loss: 0.005623\n",
      "[9,   1800] loss: 0.005667\n",
      "[9,   1820] loss: 0.005394\n",
      "[9,   1840] loss: 0.005879\n",
      "[9,   1860] loss: 0.005571\n",
      "[9,   1880] loss: 0.005679\n",
      "[9,   1900] loss: 0.005581\n",
      "[9,   1920] loss: 0.005894\n",
      "[9,   1940] loss: 0.005951\n",
      "[9,   1960] loss: 0.005728\n",
      "[9,   1980] loss: 0.005970\n",
      "[9,   2000] loss: 0.005548\n",
      "[9,   2020] loss: 0.005484\n",
      "[9,   2040] loss: 0.005487\n",
      "[9,   2060] loss: 0.005378\n",
      "[9,   2080] loss: 0.005587\n",
      "[9,   2100] loss: 0.005412\n",
      "[9,   2120] loss: 0.005605\n",
      "[9,   2140] loss: 0.005329\n",
      "[9,   2160] loss: 0.006251\n",
      "[9,   2180] loss: 0.005600\n",
      "[9,   2200] loss: 0.005342\n",
      "[9,   2220] loss: 0.005701\n",
      "[9,   2240] loss: 0.005327\n",
      "[9,   2260] loss: 0.005657\n",
      "[9,   2280] loss: 0.005405\n",
      "[9,   2300] loss: 0.005736\n",
      "[9,   2320] loss: 0.005733\n",
      "[9,   2340] loss: 0.005538\n",
      "[9,   2360] loss: 0.005527\n",
      "[9,   2380] loss: 0.005720\n",
      "[9,   2400] loss: 0.005433\n",
      "[9,   2420] loss: 0.005251\n",
      "[9,   2440] loss: 0.005484\n",
      "[9,   2460] loss: 0.005319\n",
      "[9,   2480] loss: 0.005228\n",
      "[9,   2500] loss: 0.005171\n",
      "[10,     20] loss: 0.005421\n",
      "[10,     40] loss: 0.005044\n",
      "[10,     60] loss: 0.005155\n",
      "[10,     80] loss: 0.005946\n",
      "[10,    100] loss: 0.006063\n",
      "[10,    120] loss: 0.005573\n",
      "[10,    140] loss: 0.005569\n",
      "[10,    160] loss: 0.005512\n",
      "[10,    180] loss: 0.005365\n",
      "[10,    200] loss: 0.005520\n",
      "[10,    220] loss: 0.005603\n",
      "[10,    240] loss: 0.005573\n",
      "[10,    260] loss: 0.005558\n",
      "[10,    280] loss: 0.005610\n",
      "[10,    300] loss: 0.005534\n",
      "[10,    320] loss: 0.005454\n",
      "[10,    340] loss: 0.005241\n",
      "[10,    360] loss: 0.005333\n",
      "[10,    380] loss: 0.005346\n",
      "[10,    400] loss: 0.005628\n",
      "[10,    420] loss: 0.005390\n",
      "[10,    440] loss: 0.005631\n",
      "[10,    460] loss: 0.005572\n",
      "[10,    480] loss: 0.005459\n",
      "[10,    500] loss: 0.005230\n",
      "[10,    520] loss: 0.005322\n",
      "[10,    540] loss: 0.005557\n",
      "[10,    560] loss: 0.005379\n",
      "[10,    580] loss: 0.005355\n",
      "[10,    600] loss: 0.005343\n",
      "[10,    620] loss: 0.005660\n",
      "[10,    640] loss: 0.005686\n",
      "[10,    660] loss: 0.005213\n",
      "[10,    680] loss: 0.005473\n",
      "[10,    700] loss: 0.005281\n",
      "[10,    720] loss: 0.005372\n",
      "[10,    740] loss: 0.005436\n",
      "[10,    760] loss: 0.005221\n",
      "[10,    780] loss: 0.005628\n",
      "[10,    800] loss: 0.005344\n",
      "[10,    820] loss: 0.005586\n",
      "[10,    840] loss: 0.005153\n",
      "[10,    860] loss: 0.005403\n",
      "[10,    880] loss: 0.006121\n",
      "[10,    900] loss: 0.005452\n",
      "[10,    920] loss: 0.005341\n",
      "[10,    940] loss: 0.005408\n",
      "[10,    960] loss: 0.005248\n",
      "[10,    980] loss: 0.005601\n",
      "[10,   1000] loss: 0.005223\n",
      "[10,   1020] loss: 0.005168\n",
      "[10,   1040] loss: 0.005637\n",
      "[10,   1060] loss: 0.006241\n",
      "[10,   1080] loss: 0.006128\n",
      "[10,   1100] loss: 0.005806\n",
      "[10,   1120] loss: 0.005485\n",
      "[10,   1140] loss: 0.005730\n",
      "[10,   1160] loss: 0.005677\n",
      "[10,   1180] loss: 0.005521\n",
      "[10,   1200] loss: 0.005847\n",
      "[10,   1220] loss: 0.005570\n",
      "[10,   1240] loss: 0.005643\n",
      "[10,   1260] loss: 0.005435\n",
      "[10,   1280] loss: 0.005906\n",
      "[10,   1300] loss: 0.005595\n",
      "[10,   1320] loss: 0.005356\n",
      "[10,   1340] loss: 0.005422\n",
      "[10,   1360] loss: 0.005191\n",
      "[10,   1380] loss: 0.005149\n",
      "[10,   1400] loss: 0.005236\n",
      "[10,   1420] loss: 0.005402\n",
      "[10,   1440] loss: 0.005357\n",
      "[10,   1460] loss: 0.005432\n",
      "[10,   1480] loss: 0.005468\n",
      "[10,   1500] loss: 0.005417\n",
      "[10,   1520] loss: 0.005352\n",
      "[10,   1540] loss: 0.005510\n",
      "[10,   1560] loss: 0.005628\n",
      "[10,   1580] loss: 0.005797\n",
      "[10,   1600] loss: 0.005263\n",
      "[10,   1620] loss: 0.005262\n",
      "[10,   1640] loss: 0.005475\n",
      "[10,   1660] loss: 0.005981\n",
      "[10,   1680] loss: 0.005464\n",
      "[10,   1700] loss: 0.005258\n",
      "[10,   1720] loss: 0.005607\n",
      "[10,   1740] loss: 0.005232\n",
      "[10,   1760] loss: 0.005262\n",
      "[10,   1780] loss: 0.005667\n",
      "[10,   1800] loss: 0.005479\n",
      "[10,   1820] loss: 0.005939\n",
      "[10,   1840] loss: 0.005776\n",
      "[10,   1860] loss: 0.004995\n",
      "[10,   1880] loss: 0.005737\n",
      "[10,   1900] loss: 0.005769\n",
      "[10,   1920] loss: 0.005191\n",
      "[10,   1940] loss: 0.005554\n",
      "[10,   1960] loss: 0.005457\n",
      "[10,   1980] loss: 0.005544\n",
      "[10,   2000] loss: 0.005500\n",
      "[10,   2020] loss: 0.005359\n",
      "[10,   2040] loss: 0.005138\n",
      "[10,   2060] loss: 0.005541\n",
      "[10,   2080] loss: 0.005765\n",
      "[10,   2100] loss: 0.005702\n",
      "[10,   2120] loss: 0.005398\n",
      "[10,   2140] loss: 0.005692\n",
      "[10,   2160] loss: 0.005107\n",
      "[10,   2180] loss: 0.005644\n",
      "[10,   2200] loss: 0.005261\n",
      "[10,   2220] loss: 0.005270\n",
      "[10,   2240] loss: 0.005111\n",
      "[10,   2260] loss: 0.005656\n",
      "[10,   2280] loss: 0.006115\n",
      "[10,   2300] loss: 0.005597\n",
      "[10,   2320] loss: 0.005395\n",
      "[10,   2340] loss: 0.005444\n",
      "[10,   2360] loss: 0.005447\n",
      "[10,   2380] loss: 0.005557\n",
      "[10,   2400] loss: 0.005354\n",
      "[10,   2420] loss: 0.005287\n",
      "[10,   2440] loss: 0.005287\n",
      "[10,   2460] loss: 0.005578\n",
      "[10,   2480] loss: 0.005889\n",
      "[10,   2500] loss: 0.005937\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(config.epochs):  # loop over the dataset multiple times\n",
    "\n",
    "  running_loss = 0.0\n",
    "\n",
    "  for i, (img, labels) in enumerate(dataloader):\n",
    "        \n",
    "    img = torch.as_tensor(img, dtype=torch.float32).cuda()\n",
    "\n",
    "    labels = torch.as_tensor(labels, dtype=torch.float32).cuda()    \n",
    "    labels = labels.flatten()\n",
    "    labels = torch.reshape(labels, (batch_size, 4*config.objects))\n",
    "     \n",
    "    optimizer.zero_grad()\n",
    "    out = model(img)\n",
    "        \n",
    "   \n",
    "    loss = criterion(out, labels)             \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # print statistics\n",
    "    running_loss += loss.item()\n",
    "    if i % 20 == 19:    # print every 2000 mini-batches\n",
    "        wandb.log({'loss': running_loss / 20})\n",
    "        print('[%d, %6d] loss: %.6f' %\n",
    "              (epoch + 1, i + 1, running_loss / 20))\n",
    "      \n",
    "        running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input image:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<svg height=\"250.618594pt\" version=\"1.1\" viewBox=\"0 0 251.565 250.618594\" width=\"251.565pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-06-18T23:37:19.798451</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.4.2, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 250.618594 \r\nL 251.565 250.618594 \r\nL 251.565 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 26.925 226.740469 \r\nL 244.365 226.740469 \r\nL 244.365 9.300469 \r\nL 26.925 9.300469 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g clip-path=\"url(#p14ccc702a9)\">\r\n    <image height=\"218\" id=\"imagef55af46b58\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"26.925\" xlink:href=\"data:image/png;base64,\r\niVBORw0KGgoAAAANSUhEUgAAANoAAADaCAYAAADAHVzbAAAChElEQVR4nO3VwRFAQAAEwaOE5i8zKROFOUV3BPuZ2mWMcQ3gUevsAfAHQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQILDNHsA3HOc+e8KreTQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAjckLwD1iUpw4AAAAAASUVORK5CYII=\" y=\"-8.740469\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m6b901d543f\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"28.62375\" xlink:href=\"#m6b901d543f\" y=\"226.740469\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(25.4425 241.338906)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2034 4250 \r\nQ 1547 4250 1301 3770 \r\nQ 1056 3291 1056 2328 \r\nQ 1056 1369 1301 889 \r\nQ 1547 409 2034 409 \r\nQ 2525 409 2770 889 \r\nQ 3016 1369 3016 2328 \r\nQ 3016 3291 2770 3770 \r\nQ 2525 4250 2034 4250 \r\nz\r\nM 2034 4750 \r\nQ 2819 4750 3233 4129 \r\nQ 3647 3509 3647 2328 \r\nQ 3647 1150 3233 529 \r\nQ 2819 -91 2034 -91 \r\nQ 1250 -91 836 529 \r\nQ 422 1150 422 2328 \r\nQ 422 3509 836 4129 \r\nQ 1250 4750 2034 4750 \r\nz\r\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"62.59875\" xlink:href=\"#m6b901d543f\" y=\"226.740469\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(56.23625 241.338906)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 794 531 \r\nL 1825 531 \r\nL 1825 4091 \r\nL 703 3866 \r\nL 703 4441 \r\nL 1819 4666 \r\nL 2450 4666 \r\nL 2450 531 \r\nL 3481 531 \r\nL 3481 0 \r\nL 794 0 \r\nL 794 531 \r\nz\r\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"96.57375\" xlink:href=\"#m6b901d543f\" y=\"226.740469\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(90.21125 241.338906)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 1228 531 \r\nL 3431 531 \r\nL 3431 0 \r\nL 469 0 \r\nL 469 531 \r\nQ 828 903 1448 1529 \r\nQ 2069 2156 2228 2338 \r\nQ 2531 2678 2651 2914 \r\nQ 2772 3150 2772 3378 \r\nQ 2772 3750 2511 3984 \r\nQ 2250 4219 1831 4219 \r\nQ 1534 4219 1204 4116 \r\nQ 875 4013 500 3803 \r\nL 500 4441 \r\nQ 881 4594 1212 4672 \r\nQ 1544 4750 1819 4750 \r\nQ 2544 4750 2975 4387 \r\nQ 3406 4025 3406 3419 \r\nQ 3406 3131 3298 2873 \r\nQ 3191 2616 2906 2266 \r\nQ 2828 2175 2409 1742 \r\nQ 1991 1309 1228 531 \r\nz\r\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"130.54875\" xlink:href=\"#m6b901d543f\" y=\"226.740469\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 30 -->\r\n      <g transform=\"translate(124.18625 241.338906)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2597 2516 \r\nQ 3050 2419 3304 2112 \r\nQ 3559 1806 3559 1356 \r\nQ 3559 666 3084 287 \r\nQ 2609 -91 1734 -91 \r\nQ 1441 -91 1130 -33 \r\nQ 819 25 488 141 \r\nL 488 750 \r\nQ 750 597 1062 519 \r\nQ 1375 441 1716 441 \r\nQ 2309 441 2620 675 \r\nQ 2931 909 2931 1356 \r\nQ 2931 1769 2642 2001 \r\nQ 2353 2234 1838 2234 \r\nL 1294 2234 \r\nL 1294 2753 \r\nL 1863 2753 \r\nQ 2328 2753 2575 2939 \r\nQ 2822 3125 2822 3475 \r\nQ 2822 3834 2567 4026 \r\nQ 2313 4219 1838 4219 \r\nQ 1578 4219 1281 4162 \r\nQ 984 4106 628 3988 \r\nL 628 4550 \r\nQ 988 4650 1302 4700 \r\nQ 1616 4750 1894 4750 \r\nQ 2613 4750 3031 4423 \r\nQ 3450 4097 3450 3541 \r\nQ 3450 3153 3228 2886 \r\nQ 3006 2619 2597 2516 \r\nz\r\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-33\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"164.52375\" xlink:href=\"#m6b901d543f\" y=\"226.740469\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 40 -->\r\n      <g transform=\"translate(158.16125 241.338906)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2419 4116 \r\nL 825 1625 \r\nL 2419 1625 \r\nL 2419 4116 \r\nz\r\nM 2253 4666 \r\nL 3047 4666 \r\nL 3047 1625 \r\nL 3713 1625 \r\nL 3713 1100 \r\nL 3047 1100 \r\nL 3047 0 \r\nL 2419 0 \r\nL 2419 1100 \r\nL 313 1100 \r\nL 313 1709 \r\nL 2253 4666 \r\nz\r\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-34\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"198.49875\" xlink:href=\"#m6b901d543f\" y=\"226.740469\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 50 -->\r\n      <g transform=\"translate(192.13625 241.338906)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 691 4666 \r\nL 3169 4666 \r\nL 3169 4134 \r\nL 1269 4134 \r\nL 1269 2991 \r\nQ 1406 3038 1543 3061 \r\nQ 1681 3084 1819 3084 \r\nQ 2600 3084 3056 2656 \r\nQ 3513 2228 3513 1497 \r\nQ 3513 744 3044 326 \r\nQ 2575 -91 1722 -91 \r\nQ 1428 -91 1123 -41 \r\nQ 819 9 494 109 \r\nL 494 744 \r\nQ 775 591 1075 516 \r\nQ 1375 441 1709 441 \r\nQ 2250 441 2565 725 \r\nQ 2881 1009 2881 1497 \r\nQ 2881 1984 2565 2268 \r\nQ 2250 2553 1709 2553 \r\nQ 1456 2553 1204 2497 \r\nQ 953 2441 691 2322 \r\nL 691 4666 \r\nz\r\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-35\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"232.47375\" xlink:href=\"#m6b901d543f\" y=\"226.740469\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 60 -->\r\n      <g transform=\"translate(226.11125 241.338906)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2113 2584 \r\nQ 1688 2584 1439 2293 \r\nQ 1191 2003 1191 1497 \r\nQ 1191 994 1439 701 \r\nQ 1688 409 2113 409 \r\nQ 2538 409 2786 701 \r\nQ 3034 994 3034 1497 \r\nQ 3034 2003 2786 2293 \r\nQ 2538 2584 2113 2584 \r\nz\r\nM 3366 4563 \r\nL 3366 3988 \r\nQ 3128 4100 2886 4159 \r\nQ 2644 4219 2406 4219 \r\nQ 1781 4219 1451 3797 \r\nQ 1122 3375 1075 2522 \r\nQ 1259 2794 1537 2939 \r\nQ 1816 3084 2150 3084 \r\nQ 2853 3084 3261 2657 \r\nQ 3669 2231 3669 1497 \r\nQ 3669 778 3244 343 \r\nQ 2819 -91 2113 -91 \r\nQ 1303 -91 875 529 \r\nQ 447 1150 447 2328 \r\nQ 447 3434 972 4092 \r\nQ 1497 4750 2381 4750 \r\nQ 2619 4750 2861 4703 \r\nQ 3103 4656 3366 4563 \r\nz\r\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-36\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_8\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"ma76ff7431d\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#ma76ff7431d\" y=\"10.999219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(13.5625 14.798437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#ma76ff7431d\" y=\"44.974219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(7.2 48.773437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#ma76ff7431d\" y=\"78.949219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(7.2 82.748437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#ma76ff7431d\" y=\"112.924219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 30 -->\r\n      <g transform=\"translate(7.2 116.723437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-33\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#ma76ff7431d\" y=\"146.899219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 40 -->\r\n      <g transform=\"translate(7.2 150.698437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-34\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#ma76ff7431d\" y=\"180.874219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 50 -->\r\n      <g transform=\"translate(7.2 184.673437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-35\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#ma76ff7431d\" y=\"214.849219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 60 -->\r\n      <g transform=\"translate(7.2 218.648437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-36\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 26.925 226.740469 \r\nL 26.925 9.300469 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 244.365 226.740469 \r\nL 244.365 9.300469 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 26.925 226.740469 \r\nL 244.365 226.740469 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 26.925 9.300469 \r\nL 244.365 9.300469 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p14ccc702a9\">\r\n   <rect height=\"217.44\" width=\"217.44\" x=\"26.925\" y=\"9.300469\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input box location (x, y), (w, h):\n(43, -3) (82, 15)\n--------------------------------------------------\npredicted output:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<svg height=\"250.618594pt\" version=\"1.1\" viewBox=\"0 0 251.565 250.618594\" width=\"251.565pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-06-18T23:37:19.914451</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.4.2, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 250.618594 \r\nL 251.565 250.618594 \r\nL 251.565 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 26.925 226.740469 \r\nL 244.365 226.740469 \r\nL 244.365 9.300469 \r\nL 26.925 9.300469 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g clip-path=\"url(#pcf728608a8)\">\r\n    <image height=\"218\" id=\"imagef92232bdda\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"26.925\" xlink:href=\"data:image/png;base64,\r\niVBORw0KGgoAAAANSUhEUgAAANoAAADaCAYAAADAHVzbAAAChElEQVR4nO3VwRFAQAAEQadk5i8zMROFOUV3BPuZ2jHGuBbgUevsAfAHQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDwDZ7AN9wnPvsCa/m0SAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAjcorUD1gOHI8IAAAAASUVORK5CYII=\" y=\"-8.740469\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m200d26dd1d\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"28.62375\" xlink:href=\"#m200d26dd1d\" y=\"226.740469\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(25.4425 241.338906)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2034 4250 \r\nQ 1547 4250 1301 3770 \r\nQ 1056 3291 1056 2328 \r\nQ 1056 1369 1301 889 \r\nQ 1547 409 2034 409 \r\nQ 2525 409 2770 889 \r\nQ 3016 1369 3016 2328 \r\nQ 3016 3291 2770 3770 \r\nQ 2525 4250 2034 4250 \r\nz\r\nM 2034 4750 \r\nQ 2819 4750 3233 4129 \r\nQ 3647 3509 3647 2328 \r\nQ 3647 1150 3233 529 \r\nQ 2819 -91 2034 -91 \r\nQ 1250 -91 836 529 \r\nQ 422 1150 422 2328 \r\nQ 422 3509 836 4129 \r\nQ 1250 4750 2034 4750 \r\nz\r\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"62.59875\" xlink:href=\"#m200d26dd1d\" y=\"226.740469\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(56.23625 241.338906)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 794 531 \r\nL 1825 531 \r\nL 1825 4091 \r\nL 703 3866 \r\nL 703 4441 \r\nL 1819 4666 \r\nL 2450 4666 \r\nL 2450 531 \r\nL 3481 531 \r\nL 3481 0 \r\nL 794 0 \r\nL 794 531 \r\nz\r\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"96.57375\" xlink:href=\"#m200d26dd1d\" y=\"226.740469\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(90.21125 241.338906)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 1228 531 \r\nL 3431 531 \r\nL 3431 0 \r\nL 469 0 \r\nL 469 531 \r\nQ 828 903 1448 1529 \r\nQ 2069 2156 2228 2338 \r\nQ 2531 2678 2651 2914 \r\nQ 2772 3150 2772 3378 \r\nQ 2772 3750 2511 3984 \r\nQ 2250 4219 1831 4219 \r\nQ 1534 4219 1204 4116 \r\nQ 875 4013 500 3803 \r\nL 500 4441 \r\nQ 881 4594 1212 4672 \r\nQ 1544 4750 1819 4750 \r\nQ 2544 4750 2975 4387 \r\nQ 3406 4025 3406 3419 \r\nQ 3406 3131 3298 2873 \r\nQ 3191 2616 2906 2266 \r\nQ 2828 2175 2409 1742 \r\nQ 1991 1309 1228 531 \r\nz\r\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"130.54875\" xlink:href=\"#m200d26dd1d\" y=\"226.740469\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 30 -->\r\n      <g transform=\"translate(124.18625 241.338906)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2597 2516 \r\nQ 3050 2419 3304 2112 \r\nQ 3559 1806 3559 1356 \r\nQ 3559 666 3084 287 \r\nQ 2609 -91 1734 -91 \r\nQ 1441 -91 1130 -33 \r\nQ 819 25 488 141 \r\nL 488 750 \r\nQ 750 597 1062 519 \r\nQ 1375 441 1716 441 \r\nQ 2309 441 2620 675 \r\nQ 2931 909 2931 1356 \r\nQ 2931 1769 2642 2001 \r\nQ 2353 2234 1838 2234 \r\nL 1294 2234 \r\nL 1294 2753 \r\nL 1863 2753 \r\nQ 2328 2753 2575 2939 \r\nQ 2822 3125 2822 3475 \r\nQ 2822 3834 2567 4026 \r\nQ 2313 4219 1838 4219 \r\nQ 1578 4219 1281 4162 \r\nQ 984 4106 628 3988 \r\nL 628 4550 \r\nQ 988 4650 1302 4700 \r\nQ 1616 4750 1894 4750 \r\nQ 2613 4750 3031 4423 \r\nQ 3450 4097 3450 3541 \r\nQ 3450 3153 3228 2886 \r\nQ 3006 2619 2597 2516 \r\nz\r\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-33\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"164.52375\" xlink:href=\"#m200d26dd1d\" y=\"226.740469\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 40 -->\r\n      <g transform=\"translate(158.16125 241.338906)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2419 4116 \r\nL 825 1625 \r\nL 2419 1625 \r\nL 2419 4116 \r\nz\r\nM 2253 4666 \r\nL 3047 4666 \r\nL 3047 1625 \r\nL 3713 1625 \r\nL 3713 1100 \r\nL 3047 1100 \r\nL 3047 0 \r\nL 2419 0 \r\nL 2419 1100 \r\nL 313 1100 \r\nL 313 1709 \r\nL 2253 4666 \r\nz\r\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-34\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"198.49875\" xlink:href=\"#m200d26dd1d\" y=\"226.740469\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 50 -->\r\n      <g transform=\"translate(192.13625 241.338906)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 691 4666 \r\nL 3169 4666 \r\nL 3169 4134 \r\nL 1269 4134 \r\nL 1269 2991 \r\nQ 1406 3038 1543 3061 \r\nQ 1681 3084 1819 3084 \r\nQ 2600 3084 3056 2656 \r\nQ 3513 2228 3513 1497 \r\nQ 3513 744 3044 326 \r\nQ 2575 -91 1722 -91 \r\nQ 1428 -91 1123 -41 \r\nQ 819 9 494 109 \r\nL 494 744 \r\nQ 775 591 1075 516 \r\nQ 1375 441 1709 441 \r\nQ 2250 441 2565 725 \r\nQ 2881 1009 2881 1497 \r\nQ 2881 1984 2565 2268 \r\nQ 2250 2553 1709 2553 \r\nQ 1456 2553 1204 2497 \r\nQ 953 2441 691 2322 \r\nL 691 4666 \r\nz\r\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-35\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"232.47375\" xlink:href=\"#m200d26dd1d\" y=\"226.740469\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 60 -->\r\n      <g transform=\"translate(226.11125 241.338906)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2113 2584 \r\nQ 1688 2584 1439 2293 \r\nQ 1191 2003 1191 1497 \r\nQ 1191 994 1439 701 \r\nQ 1688 409 2113 409 \r\nQ 2538 409 2786 701 \r\nQ 3034 994 3034 1497 \r\nQ 3034 2003 2786 2293 \r\nQ 2538 2584 2113 2584 \r\nz\r\nM 3366 4563 \r\nL 3366 3988 \r\nQ 3128 4100 2886 4159 \r\nQ 2644 4219 2406 4219 \r\nQ 1781 4219 1451 3797 \r\nQ 1122 3375 1075 2522 \r\nQ 1259 2794 1537 2939 \r\nQ 1816 3084 2150 3084 \r\nQ 2853 3084 3261 2657 \r\nQ 3669 2231 3669 1497 \r\nQ 3669 778 3244 343 \r\nQ 2819 -91 2113 -91 \r\nQ 1303 -91 875 529 \r\nQ 447 1150 447 2328 \r\nQ 447 3434 972 4092 \r\nQ 1497 4750 2381 4750 \r\nQ 2619 4750 2861 4703 \r\nQ 3103 4656 3366 4563 \r\nz\r\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-36\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_8\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"mabc6af8c57\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mabc6af8c57\" y=\"10.999219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(13.5625 14.798437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mabc6af8c57\" y=\"44.974219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(7.2 48.773437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mabc6af8c57\" y=\"78.949219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(7.2 82.748437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mabc6af8c57\" y=\"112.924219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 30 -->\r\n      <g transform=\"translate(7.2 116.723437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-33\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mabc6af8c57\" y=\"146.899219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 40 -->\r\n      <g transform=\"translate(7.2 150.698437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-34\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mabc6af8c57\" y=\"180.874219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 50 -->\r\n      <g transform=\"translate(7.2 184.673437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-35\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mabc6af8c57\" y=\"214.849219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 60 -->\r\n      <g transform=\"translate(7.2 218.648437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-36\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 26.925 226.740469 \r\nL 26.925 9.300469 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 244.365 226.740469 \r\nL 244.365 9.300469 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 26.925 226.740469 \r\nL 244.365 226.740469 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 26.925 9.300469 \r\nL 244.365 9.300469 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"pcf728608a8\">\r\n   <rect height=\"217.44\" width=\"217.44\" x=\"26.925\" y=\"9.300469\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "predicted box location (x, y), (w, h):\n(43, -8) (70, 14)\n--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# testing the trained model\n",
    "\n",
    "model = model.eval()\n",
    "\n",
    "np.random.seed(np.random.randint(0, 99999))\n",
    "\n",
    "image = np.zeros((size,size, 3), np.uint8)\n",
    "labels = []\n",
    "model = model.cuda()\n",
    "\n",
    "point_x = int(np.random.rand() * size)\n",
    "point_y = int(np.random.rand() * size)\n",
    "size_x = int(np.random.rand() * size)\n",
    "size_y = int(np.random.rand() * size)       \n",
    "r, g, b = int(np.random.rand()*255), int(np.random.rand()*255), int(np.random.rand() * 255)  \n",
    "\n",
    "cv2.rectangle(image, (int(point_x - size_x/2), int(point_y - size_y/2)), (int(point_x + size_x/2), int(point_y + size_y/2)), (r, g, b), -1)            \n",
    "\n",
    "print(\"input image:\")\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "print(\"input box location (x, y), (w, h):\")\n",
    "print((int(point_x - size_x/2), int(point_y - size_y/2)), (int(point_x + size_x/2), int(point_y + size_y/2)))\n",
    "\n",
    "image_tensor = torch.as_tensor(image, dtype=torch.float32)\n",
    "\n",
    "image_tensor = torch.unsqueeze(image_tensor, 0).cuda()    \n",
    "\n",
    "image2 = np.ones((size,size, 3), np.uint8)\n",
    "\n",
    "t1 = time.time()\n",
    "for i in range(1):\n",
    "  out = model(image_tensor)\n",
    "  out = out[0].detach().cpu().numpy()\n",
    "  \n",
    "out_point_x = int(out[0] * size)\n",
    "out_point_y = int(out[1] * size)\n",
    "out_size_x = int(out[2] * size)\n",
    "out_size_y = int(out[3] * size)\n",
    "\n",
    "cv2.rectangle(image2, (int(out_point_x - out_size_x/2), int(out_point_y - out_size_y/2)), (int(out_point_x + out_size_x/2), int(out_point_y + out_size_y/2)), (r, g, b), -1)            \n",
    "\n",
    "print(\"-\"*50)\n",
    "print(\"predicted output:\")\n",
    "plt.imshow(image2)\n",
    "plt.show()\n",
    "print(\"predicted box location (x, y), (w, h):\")\n",
    "print((int(out_point_x - out_size_x/2), int(out_point_y - out_size_y/2)), (int(out_point_x + out_size_x/2), int(out_point_y + out_size_y/2)))\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}