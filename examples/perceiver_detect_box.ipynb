{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit (windows store)"
  },
  "interpreter": {
   "hash": "3677cddbc1f82f62e0ae465bd57603547b71c057823345d3a3ffe3c045472941"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 21.1.2 is available.\nYou should consider upgrading via the 'C:\\Users\\Ryan\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if os.name == 'nt':\n",
    "    !pip install wandb torch==1.8.1+cu111 -f https://download.pytorch.org/whl/torch_stable.html opencv-python torchvision %CD%\\.. --quiet\n",
    "else:\n",
    "    !pip install wandb torch==1.8.1+cu111 -f https://download.pytorch.org/whl/torch_stable.html opencv-python torchvision $(pwd)/.. --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import torch\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from matplotlib import pyplot as plt\n",
    "from perceiver_pytorch import Perceiver\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "\u001b[1;31mInit signature:\u001b[0m\n",
      "\u001b[0mPerceiver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mnum_freq_bands\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdepth\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmax_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mfreq_base\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0minput_channels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0minput_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mnum_latents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mlatent_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mcross_heads\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mlatent_heads\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mcross_dim_head\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mlatent_dim_head\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mattn_dropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mff_dropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mfnet\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mweight_tie_layers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mfourier_encode_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mself_per_cross_attn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mself_attn_rel_pos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m     \n",
      "Base class for all neural network modules.\n",
      "\n",
      "Your models should also subclass this class.\n",
      "\n",
      "Modules can also contain other Modules, allowing to nest them in\n",
      "a tree structure. You can assign the submodules as regular attributes::\n",
      "\n",
      "    import torch.nn as nn\n",
      "    import torch.nn.functional as F\n",
      "\n",
      "    class Model(nn.Module):\n",
      "        def __init__(self):\n",
      "            super(Model, self).__init__()\n",
      "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
      "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
      "\n",
      "        def forward(self, x):\n",
      "            x = F.relu(self.conv1(x))\n",
      "            return F.relu(self.conv2(x))\n",
      "\n",
      "Submodules assigned in this way will be registered, and will have their\n",
      "parameters converted too when you call :meth:`to`, etc.\n",
      "\n",
      ":ivar training: Boolean represents whether this module is in training or\n",
      "                evaluation mode.\n",
      ":vartype training: bool\n",
      "\u001b[1;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "\u001b[1;31mFile:\u001b[0m           c:\\users\\ryan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages\\perceiver_pytorch\\perceiver_pytorch.py\n",
      "\u001b[1;31mType:\u001b[0m           type\n",
      "\u001b[1;31mSubclasses:\u001b[0m     \n"
     ],
     "name": "stdout"
    }
   ],
   "source": [
    "?Perceiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Finishing last run (ID:2oj641xk) before initializing another..."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 13252<br/>Program ended successfully."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find user logs for this run at: <code>c:\\Users\\Ryan\\Repositories\\perceiver-pytorch\\wandb\\run-20210619_003918-2oj641xk\\logs\\debug.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find internal logs for this run at: <code>c:\\Users\\Ryan\\Repositories\\perceiver-pytorch\\wandb\\run-20210619_003918-2oj641xk\\logs\\debug-internal.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    <br/>Synced <strong style=\"color:#cdcd00\">floral-frost-5</strong>: <a href=\"https://wandb.ai/rosenblatt/fnet-preceiver/runs/2oj641xk\" target=\"_blank\">https://wandb.ai/rosenblatt/fnet-preceiver/runs/2oj641xk</a><br/>\n                "
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "...Successfully finished last run (ID:2oj641xk). Initializing new run:<br/><br/>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.32<br/>\n                Syncing run <strong style=\"color:#cdcd00\">pretty-planet-6</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/rosenblatt/fnet-preceiver\" target=\"_blank\">https://wandb.ai/rosenblatt/fnet-preceiver</a><br/>\n                Run page: <a href=\"https://wandb.ai/rosenblatt/fnet-preceiver/runs/3ac1ks0k\" target=\"_blank\">https://wandb.ai/rosenblatt/fnet-preceiver/runs/3ac1ks0k</a><br/>\n                Run data is saved locally in <code>c:\\Users\\Ryan\\Repositories\\perceiver-pytorch\\wandb\\run-20210619_003925-3ac1ks0k</code><br/><br/>\n            "
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/html": "<h1>Run(3ac1ks0k)</h1><iframe src=\"https://wandb.ai/rosenblatt/fnet-preceiver/runs/3ac1ks0k\" style=\"border:none;width:100%;height:400px\"></iframe>",
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x2288fe21070>"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "# 1. Start a W&B run\n",
    "wandb.init(project='fnet-preceiver', entity='rosenblatt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Save model inputs and hyperparameters\n",
    "config = wandb.config\n",
    "config.learning_rate = 1e-3\n",
    "config.batch_size = 128\n",
    "config.size = 64\n",
    "config.objects = 1\n",
    "config.epochs = 10\n",
    "config.fnet = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Perceiver(\n",
    "    input_channels = 3,          # number of channels for each token of the input\n",
    "    input_axis = 2,              # number of axis for input data (2 for images, 3 for video)\n",
    "    num_freq_bands = 4,          # number of freq bands, with original value (2 * K + 1)\n",
    "    max_freq = 5.,              # maximum frequency, hyperparameter depending on how fine the data is\n",
    "    depth = 2,                   # depth of net\n",
    "    num_latents = 32,           # number of latents, or induced set points, or centroids. different papers giving it different names\n",
    "    # cross_dim = 32,             # cross attention dimension\n",
    "    latent_dim = 32,            # latent dimension\n",
    "    cross_heads = 1,             # number of heads for cross attention. paper said 1\n",
    "    latent_heads = 4,            # number of heads for latent self attention, 8\n",
    "    cross_dim_head = 32,\n",
    "    latent_dim_head = 32,\n",
    "    num_classes = 4*config.objects,          # output number of classes\n",
    "    attn_dropout = 0.5,\n",
    "    ff_dropout = 0.5,\n",
    "    fnet = config.fnet, # whether to replace latent transformers with FNets\n",
    "    weight_tie_layers = True    # whether to weight tie layers (optional, as indicated in the diagram)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model on very easy object detection problem\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self):   \n",
    "      pass\n",
    "      \n",
    "    def __len__(self):\n",
    "        return 320000\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = np.zeros((config.size,config.size,3), np.uint8)\n",
    "        labels = []\n",
    "\n",
    "        for i in range(config.objects):\n",
    "          if (np.random.rand() > 0.02):\n",
    "            point_x = int(np.random.rand() * config.size)\n",
    "            point_y = int(np.random.rand() * config.size)\n",
    "            size_x = int(np.random.rand() * config.size)\n",
    "            size_y = int(np.random.rand() * config.size)       \n",
    "            r, g, b = int(np.random.rand()*255), int(np.random.rand()*255), int(np.random.rand() * 255)  \n",
    "            try:\n",
    "              cv2.rectangle(image, (int(point_x - size_x/2), int(point_y - size_y/2)), (int(point_x + size_x/2), int(point_y + size_y/2)), (r, g, b), -1)            \n",
    "              labels.append((point_x/config.size, point_y/config.size, size_x/config.size, size_y/config.size))\n",
    "            except Exception as e:\n",
    "              print(e)\n",
    "              labels.append((0,0,0,0))\n",
    "          else:            \n",
    "            labels.append((0,0,0,0))            \n",
    "                  \n",
    "        labels = torch.as_tensor(labels, dtype=torch.float32)\n",
    "        image = torch.as_tensor(image, dtype=torch.float32)\n",
    "\n",
    "        return (image, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Perceiver(\n",
       "  (layers): ModuleList(\n",
       "    (0): ModuleList(\n",
       "      (0): PreNorm(\n",
       "        (fn): Attention(\n",
       "          (to_q): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (to_kv): Linear(in_features=21, out_features=64, bias=False)\n",
       "          (to_out): Sequential(\n",
       "            (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (1): Dropout(p=0.5, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm_context): LayerNorm((21,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): PreNorm(\n",
       "        (fn): FeedForward(\n",
       "          (net): Sequential(\n",
       "            (0): Linear(in_features=32, out_features=256, bias=True)\n",
       "            (1): GEGLU()\n",
       "            (2): Dropout(p=0.5, inplace=False)\n",
       "            (3): Linear(in_features=128, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): PreNorm(\n",
       "            (fn): FNetBlock()\n",
       "            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): PreNorm(\n",
       "            (fn): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Linear(in_features=32, out_features=256, bias=True)\n",
       "                (1): GEGLU()\n",
       "                (2): Dropout(p=0.5, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=32, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): ModuleList(\n",
       "      (0): PreNorm(\n",
       "        (fn): Attention(\n",
       "          (to_q): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (to_kv): Linear(in_features=21, out_features=64, bias=False)\n",
       "          (to_out): Sequential(\n",
       "            (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (1): Dropout(p=0.5, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm_context): LayerNorm((21,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): PreNorm(\n",
       "        (fn): FeedForward(\n",
       "          (net): Sequential(\n",
       "            (0): Linear(in_features=32, out_features=256, bias=True)\n",
       "            (1): GEGLU()\n",
       "            (2): Dropout(p=0.5, inplace=False)\n",
       "            (3): Linear(in_features=128, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): PreNorm(\n",
       "            (fn): FNetBlock()\n",
       "            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): PreNorm(\n",
       "            (fn): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Linear(in_features=32, out_features=256, bias=True)\n",
       "                (1): GEGLU()\n",
       "                (2): Dropout(p=0.5, inplace=False)\n",
       "                (3): Linear(in_features=128, out_features=32, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (to_logits): Sequential(\n",
       "    (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=32, out_features=4, bias=True)\n",
       "  )\n",
       "  (sinu_emb): SinusoidalEmbeddings()\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "criterion = torch.nn.MSELoss().cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=config.learning_rate, amsgrad=True)\n",
    "\n",
    "dataset = CustomDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "# wandb.watch(model)\n",
    "model.cuda()\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "62\n",
      "[5,    300] loss: 0.007894\n",
      "[5,    320] loss: 0.007801\n",
      "[5,    340] loss: 0.007163\n",
      "[5,    360] loss: 0.007232\n",
      "[5,    380] loss: 0.008544\n",
      "[5,    400] loss: 0.007852\n",
      "[5,    420] loss: 0.007680\n",
      "[5,    440] loss: 0.007667\n",
      "[5,    460] loss: 0.007814\n",
      "[5,    480] loss: 0.007299\n",
      "[5,    500] loss: 0.007754\n",
      "[5,    520] loss: 0.007567\n",
      "[5,    540] loss: 0.007682\n",
      "[5,    560] loss: 0.007263\n",
      "[5,    580] loss: 0.007317\n",
      "[5,    600] loss: 0.007578\n",
      "[5,    620] loss: 0.008173\n",
      "[5,    640] loss: 0.007881\n",
      "[5,    660] loss: 0.007250\n",
      "[5,    680] loss: 0.007566\n",
      "[5,    700] loss: 0.007310\n",
      "[5,    720] loss: 0.007579\n",
      "[5,    740] loss: 0.007539\n",
      "[5,    760] loss: 0.006783\n",
      "[5,    780] loss: 0.007188\n",
      "[5,    800] loss: 0.007233\n",
      "[5,    820] loss: 0.007577\n",
      "[5,    840] loss: 0.008161\n",
      "[5,    860] loss: 0.008574\n",
      "[5,    880] loss: 0.007838\n",
      "[5,    900] loss: 0.006996\n",
      "[5,    920] loss: 0.007362\n",
      "[5,    940] loss: 0.007256\n",
      "[5,    960] loss: 0.007617\n",
      "[5,    980] loss: 0.007228\n",
      "[5,   1000] loss: 0.007528\n",
      "[5,   1020] loss: 0.007878\n",
      "[5,   1040] loss: 0.011698\n",
      "[5,   1060] loss: 0.008292\n",
      "[5,   1080] loss: 0.006984\n",
      "[5,   1100] loss: 0.008145\n",
      "[5,   1120] loss: 0.007607\n",
      "[5,   1140] loss: 0.007012\n",
      "[5,   1160] loss: 0.008071\n",
      "[5,   1180] loss: 0.007804\n",
      "[5,   1200] loss: 0.006915\n",
      "[5,   1220] loss: 0.007761\n",
      "[5,   1240] loss: 0.007447\n",
      "[5,   1260] loss: 0.007050\n",
      "[5,   1280] loss: 0.006964\n",
      "[5,   1300] loss: 0.007417\n",
      "[5,   1320] loss: 0.007039\n",
      "[5,   1340] loss: 0.007591\n",
      "[5,   1360] loss: 0.007338\n",
      "[5,   1380] loss: 0.007449\n",
      "[5,   1400] loss: 0.008324\n",
      "[5,   1420] loss: 0.007547\n",
      "[5,   1440] loss: 0.007030\n",
      "[5,   1460] loss: 0.006876\n",
      "[5,   1480] loss: 0.007455\n",
      "[5,   1500] loss: 0.007029\n",
      "[5,   1520] loss: 0.007774\n",
      "[5,   1540] loss: 0.008106\n",
      "[5,   1560] loss: 0.008060\n",
      "[5,   1580] loss: 0.006925\n",
      "[5,   1600] loss: 0.006935\n",
      "[5,   1620] loss: 0.007195\n",
      "[5,   1640] loss: 0.007462\n",
      "[5,   1660] loss: 0.007058\n",
      "[5,   1680] loss: 0.007226\n",
      "[5,   1700] loss: 0.006887\n",
      "[5,   1720] loss: 0.006740\n",
      "[5,   1740] loss: 0.007498\n",
      "[5,   1760] loss: 0.007630\n",
      "[5,   1780] loss: 0.007636\n",
      "[5,   1800] loss: 0.007770\n",
      "[5,   1820] loss: 0.006732\n",
      "[5,   1840] loss: 0.007684\n",
      "[5,   1860] loss: 0.007428\n",
      "[5,   1880] loss: 0.007805\n",
      "[5,   1900] loss: 0.007346\n",
      "[5,   1920] loss: 0.007850\n",
      "[5,   1940] loss: 0.006905\n",
      "[5,   1960] loss: 0.007243\n",
      "[5,   1980] loss: 0.007923\n",
      "[5,   2000] loss: 0.007689\n",
      "[5,   2020] loss: 0.006928\n",
      "[5,   2040] loss: 0.006933\n",
      "[5,   2060] loss: 0.007198\n",
      "[5,   2080] loss: 0.008022\n",
      "[5,   2100] loss: 0.007535\n",
      "[5,   2120] loss: 0.007467\n",
      "[5,   2140] loss: 0.007191\n",
      "[5,   2160] loss: 0.006931\n",
      "[5,   2180] loss: 0.007229\n",
      "[5,   2200] loss: 0.007451\n",
      "[5,   2220] loss: 0.007833\n",
      "[5,   2240] loss: 0.007381\n",
      "[5,   2260] loss: 0.007056\n",
      "[5,   2280] loss: 0.007219\n",
      "[5,   2300] loss: 0.007215\n",
      "[5,   2320] loss: 0.007184\n",
      "[5,   2340] loss: 0.007426\n",
      "[5,   2360] loss: 0.007262\n",
      "[5,   2380] loss: 0.006742\n",
      "[5,   2400] loss: 0.006549\n",
      "[5,   2420] loss: 0.008117\n",
      "[5,   2440] loss: 0.007897\n",
      "[5,   2460] loss: 0.007530\n",
      "[5,   2480] loss: 0.007045\n",
      "[5,   2500] loss: 0.006905\n",
      "[6,     20] loss: 0.007145\n",
      "[6,     40] loss: 0.007206\n",
      "[6,     60] loss: 0.007304\n",
      "[6,     80] loss: 0.007134\n",
      "[6,    100] loss: 0.007393\n",
      "[6,    120] loss: 0.007621\n",
      "[6,    140] loss: 0.008538\n",
      "[6,    160] loss: 0.007966\n",
      "[6,    180] loss: 0.006866\n",
      "[6,    200] loss: 0.008189\n",
      "[6,    220] loss: 0.006935\n",
      "[6,    240] loss: 0.007241\n",
      "[6,    260] loss: 0.007871\n",
      "[6,    280] loss: 0.007700\n",
      "[6,    300] loss: 0.006785\n",
      "[6,    320] loss: 0.007003\n",
      "[6,    340] loss: 0.006968\n",
      "[6,    360] loss: 0.007609\n",
      "[6,    380] loss: 0.007737\n",
      "[6,    400] loss: 0.007286\n",
      "[6,    420] loss: 0.007033\n",
      "[6,    440] loss: 0.006764\n",
      "[6,    460] loss: 0.007052\n",
      "[6,    480] loss: 0.007478\n",
      "[6,    500] loss: 0.007648\n",
      "[6,    520] loss: 0.008251\n",
      "[6,    540] loss: 0.007576\n",
      "[6,    560] loss: 0.007087\n",
      "[6,    580] loss: 0.007507\n",
      "[6,    600] loss: 0.007825\n",
      "[6,    620] loss: 0.007222\n",
      "[6,    640] loss: 0.007168\n",
      "[6,    660] loss: 0.006846\n",
      "[6,    680] loss: 0.006858\n",
      "[6,    700] loss: 0.007098\n",
      "[6,    720] loss: 0.007564\n",
      "[6,    740] loss: 0.007718\n",
      "[6,    760] loss: 0.007267\n",
      "[6,    780] loss: 0.007190\n",
      "[6,    800] loss: 0.006907\n",
      "[6,    820] loss: 0.006726\n",
      "[6,    840] loss: 0.006931\n",
      "[6,    860] loss: 0.007856\n",
      "[6,    880] loss: 0.006994\n",
      "[6,    900] loss: 0.007055\n",
      "[6,    920] loss: 0.007152\n",
      "[6,    940] loss: 0.007286\n",
      "[6,    960] loss: 0.006829\n",
      "[6,    980] loss: 0.006708\n",
      "[6,   1000] loss: 0.007299\n",
      "[6,   1020] loss: 0.007278\n",
      "[6,   1040] loss: 0.006768\n",
      "[6,   1060] loss: 0.006709\n",
      "[6,   1080] loss: 0.006647\n",
      "[6,   1100] loss: 0.006697\n",
      "[6,   1120] loss: 0.007022\n",
      "[6,   1140] loss: 0.007271\n",
      "[6,   1160] loss: 0.007289\n",
      "[6,   1180] loss: 0.007134\n",
      "[6,   1200] loss: 0.007397\n",
      "[6,   1220] loss: 0.006973\n",
      "[6,   1240] loss: 0.007235\n",
      "[6,   1260] loss: 0.006980\n",
      "[6,   1280] loss: 0.006984\n",
      "[6,   1300] loss: 0.006903\n",
      "[6,   1320] loss: 0.007129\n",
      "[6,   1340] loss: 0.007669\n",
      "[6,   1360] loss: 0.007149\n",
      "[6,   1380] loss: 0.007099\n",
      "[6,   1400] loss: 0.006988\n",
      "[6,   1420] loss: 0.006528\n",
      "[6,   1440] loss: 0.006689\n",
      "[6,   1460] loss: 0.007182\n",
      "[6,   1480] loss: 0.007246\n",
      "[6,   1500] loss: 0.007033\n",
      "[6,   1520] loss: 0.006381\n",
      "[6,   1540] loss: 0.006634\n",
      "[6,   1560] loss: 0.006426\n",
      "[6,   1580] loss: 0.008059\n",
      "[6,   1600] loss: 0.007887\n",
      "[6,   1620] loss: 0.006979\n",
      "[6,   1640] loss: 0.007336\n",
      "[6,   1660] loss: 0.006898\n",
      "[6,   1680] loss: 0.008131\n",
      "[6,   1700] loss: 0.006258\n",
      "[6,   1720] loss: 0.006503\n",
      "[6,   1740] loss: 0.007033\n",
      "[6,   1760] loss: 0.006982\n",
      "[6,   1780] loss: 0.006686\n",
      "[6,   1800] loss: 0.006466\n",
      "[6,   1820] loss: 0.006605\n",
      "[6,   1840] loss: 0.006682\n",
      "[6,   1860] loss: 0.007172\n",
      "[6,   1880] loss: 0.007061\n",
      "[6,   1900] loss: 0.008420\n",
      "[6,   1920] loss: 0.006766\n",
      "[6,   1940] loss: 0.006593\n",
      "[6,   1960] loss: 0.007099\n",
      "[6,   1980] loss: 0.007116\n",
      "[6,   2000] loss: 0.006980\n",
      "[6,   2020] loss: 0.007036\n",
      "[6,   2040] loss: 0.007053\n",
      "[6,   2060] loss: 0.006772\n",
      "[6,   2080] loss: 0.006685\n",
      "[6,   2100] loss: 0.006909\n",
      "[6,   2120] loss: 0.006536\n",
      "[6,   2140] loss: 0.007018\n",
      "[6,   2160] loss: 0.007402\n",
      "[6,   2180] loss: 0.007222\n",
      "[6,   2200] loss: 0.007188\n",
      "[6,   2220] loss: 0.007278\n",
      "[6,   2240] loss: 0.007004\n",
      "[6,   2260] loss: 0.010749\n",
      "[6,   2280] loss: 0.008430\n",
      "[6,   2300] loss: 0.006895\n",
      "[6,   2320] loss: 0.006657\n",
      "[6,   2340] loss: 0.006990\n",
      "[6,   2360] loss: 0.007288\n",
      "[6,   2380] loss: 0.007407\n",
      "[6,   2400] loss: 0.007072\n",
      "[6,   2420] loss: 0.007161\n",
      "[6,   2440] loss: 0.007316\n",
      "[6,   2460] loss: 0.006934\n",
      "[6,   2480] loss: 0.006765\n",
      "[6,   2500] loss: 0.007096\n",
      "[7,     20] loss: 0.006818\n",
      "[7,     40] loss: 0.006641\n",
      "[7,     60] loss: 0.006474\n",
      "[7,     80] loss: 0.006663\n",
      "[7,    100] loss: 0.006452\n",
      "[7,    120] loss: 0.006330\n",
      "[7,    140] loss: 0.007112\n",
      "[7,    160] loss: 0.006651\n",
      "[7,    180] loss: 0.006688\n",
      "[7,    200] loss: 0.006883\n",
      "[7,    220] loss: 0.006476\n",
      "[7,    240] loss: 0.007171\n",
      "[7,    260] loss: 0.006991\n",
      "[7,    280] loss: 0.007064\n",
      "[7,    300] loss: 0.006860\n",
      "[7,    320] loss: 0.007406\n",
      "[7,    340] loss: 0.007075\n",
      "[7,    360] loss: 0.006738\n",
      "[7,    380] loss: 0.006611\n",
      "[7,    400] loss: 0.007282\n",
      "[7,    420] loss: 0.007074\n",
      "[7,    440] loss: 0.006872\n",
      "[7,    460] loss: 0.006445\n",
      "[7,    480] loss: 0.006350\n",
      "[7,    500] loss: 0.006875\n",
      "[7,    520] loss: 0.007301\n",
      "[7,    540] loss: 0.007418\n",
      "[7,    560] loss: 0.007253\n",
      "[7,    580] loss: 0.007538\n",
      "[7,    600] loss: 0.006642\n",
      "[7,    620] loss: 0.006710\n",
      "[7,    640] loss: 0.006281\n",
      "[7,    660] loss: 0.006518\n",
      "[7,    680] loss: 0.006415\n",
      "[7,    700] loss: 0.007161\n",
      "[7,    720] loss: 0.007642\n",
      "[7,    740] loss: 0.007286\n",
      "[7,    760] loss: 0.006843\n",
      "[7,    780] loss: 0.006762\n",
      "[7,    800] loss: 0.007741\n",
      "[7,    820] loss: 0.006728\n",
      "[7,    840] loss: 0.007568\n",
      "[7,    860] loss: 0.007206\n",
      "[7,    880] loss: 0.006788\n",
      "[7,    900] loss: 0.006359\n",
      "[7,    920] loss: 0.007198\n",
      "[7,    940] loss: 0.006744\n",
      "[7,    960] loss: 0.006560\n",
      "[7,    980] loss: 0.006573\n",
      "[7,   1000] loss: 0.006736\n",
      "[7,   1020] loss: 0.006688\n",
      "[7,   1040] loss: 0.007039\n",
      "[7,   1060] loss: 0.006875\n",
      "[7,   1080] loss: 0.006585\n",
      "[7,   1100] loss: 0.006351\n",
      "[7,   1120] loss: 0.007993\n",
      "[7,   1140] loss: 0.008157\n",
      "[7,   1160] loss: 0.007028\n",
      "[7,   1180] loss: 0.006762\n",
      "[7,   1200] loss: 0.006648\n",
      "[7,   1220] loss: 0.007983\n",
      "[7,   1240] loss: 0.008006\n",
      "[7,   1260] loss: 0.006525\n",
      "[7,   1280] loss: 0.006517\n",
      "[7,   1300] loss: 0.006579\n",
      "[7,   1320] loss: 0.006708\n",
      "[7,   1340] loss: 0.006437\n",
      "[7,   1360] loss: 0.006313\n",
      "[7,   1380] loss: 0.006568\n",
      "[7,   1400] loss: 0.006800\n",
      "[7,   1420] loss: 0.006443\n",
      "[7,   1440] loss: 0.006411\n",
      "[7,   1460] loss: 0.008239\n",
      "[7,   1480] loss: 0.007385\n",
      "[7,   1500] loss: 0.007185\n",
      "[7,   1520] loss: 0.007194\n",
      "[7,   1540] loss: 0.007631\n",
      "[7,   1560] loss: 0.007360\n",
      "[7,   1580] loss: 0.006801\n",
      "[7,   1600] loss: 0.006902\n",
      "[7,   1620] loss: 0.006954\n",
      "[7,   1640] loss: 0.007068\n",
      "[7,   1660] loss: 0.006922\n",
      "[7,   1680] loss: 0.007122\n",
      "[7,   1700] loss: 0.007317\n",
      "[7,   1720] loss: 0.006396\n",
      "[7,   1740] loss: 0.006676\n",
      "[7,   1760] loss: 0.006966\n",
      "[7,   1780] loss: 0.007513\n",
      "[7,   1800] loss: 0.006889\n",
      "[7,   1820] loss: 0.006860\n",
      "[7,   1840] loss: 0.006586\n",
      "[7,   1860] loss: 0.006193\n",
      "[7,   1880] loss: 0.008171\n",
      "[7,   1900] loss: 0.007730\n",
      "[7,   1920] loss: 0.006984\n",
      "[7,   1940] loss: 0.006568\n",
      "[7,   1960] loss: 0.006577\n",
      "[7,   1980] loss: 0.008098\n",
      "[7,   2000] loss: 0.006307\n",
      "[7,   2020] loss: 0.007309\n",
      "[7,   2040] loss: 0.007184\n",
      "[7,   2060] loss: 0.007393\n",
      "[7,   2080] loss: 0.007302\n",
      "[7,   2100] loss: 0.007102\n",
      "[7,   2120] loss: 0.007097\n",
      "[7,   2140] loss: 0.006932\n",
      "[7,   2160] loss: 0.006647\n",
      "[7,   2180] loss: 0.006549\n",
      "[7,   2200] loss: 0.007269\n",
      "[7,   2220] loss: 0.006791\n",
      "[7,   2240] loss: 0.006696\n",
      "[7,   2260] loss: 0.006746\n",
      "[7,   2280] loss: 0.006824\n",
      "[7,   2300] loss: 0.006975\n",
      "[7,   2320] loss: 0.007358\n",
      "[7,   2340] loss: 0.006811\n",
      "[7,   2360] loss: 0.006838\n",
      "[7,   2380] loss: 0.007176\n",
      "[7,   2400] loss: 0.006815\n",
      "[7,   2420] loss: 0.006938\n",
      "[7,   2440] loss: 0.007076\n",
      "[7,   2460] loss: 0.006597\n",
      "[7,   2480] loss: 0.006878\n",
      "[7,   2500] loss: 0.006844\n",
      "[8,     20] loss: 0.007313\n",
      "[8,     40] loss: 0.007003\n",
      "[8,     60] loss: 0.006689\n",
      "[8,     80] loss: 0.007466\n",
      "[8,    100] loss: 0.006737\n",
      "[8,    120] loss: 0.006168\n",
      "[8,    140] loss: 0.006290\n",
      "[8,    160] loss: 0.006110\n",
      "[8,    180] loss: 0.006219\n",
      "[8,    200] loss: 0.007184\n",
      "[8,    220] loss: 0.007182\n",
      "[8,    240] loss: 0.006955\n",
      "[8,    260] loss: 0.007186\n",
      "[8,    280] loss: 0.007008\n",
      "[8,    300] loss: 0.006416\n",
      "[8,    320] loss: 0.006496\n",
      "[8,    340] loss: 0.006708\n",
      "[8,    360] loss: 0.006720\n",
      "[8,    380] loss: 0.006939\n",
      "[8,    400] loss: 0.006771\n",
      "[8,    420] loss: 0.006874\n",
      "[8,    440] loss: 0.006997\n",
      "[8,    460] loss: 0.006733\n",
      "[8,    480] loss: 0.006353\n",
      "[8,    500] loss: 0.006263\n",
      "[8,    520] loss: 0.006776\n",
      "[8,    540] loss: 0.006321\n",
      "[8,    560] loss: 0.006593\n",
      "[8,    580] loss: 0.006311\n",
      "[8,    600] loss: 0.006760\n",
      "[8,    620] loss: 0.006057\n",
      "[8,    640] loss: 0.006307\n",
      "[8,    660] loss: 0.006660\n",
      "[8,    680] loss: 0.006882\n",
      "[8,    700] loss: 0.006429\n",
      "[8,    720] loss: 0.006656\n",
      "[8,    740] loss: 0.006360\n",
      "[8,    760] loss: 0.006220\n",
      "[8,    780] loss: 0.006243\n",
      "[8,    800] loss: 0.006595\n",
      "[8,    820] loss: 0.006374\n",
      "[8,    840] loss: 0.006007\n",
      "[8,    860] loss: 0.006006\n",
      "[8,    880] loss: 0.006323\n",
      "[8,    900] loss: 0.006894\n",
      "[8,    920] loss: 0.006498\n",
      "[8,    940] loss: 0.006495\n",
      "[8,    960] loss: 0.006569\n",
      "[8,    980] loss: 0.006741\n",
      "[8,   1000] loss: 0.006071\n",
      "[8,   1020] loss: 0.006769\n",
      "[8,   1040] loss: 0.006767\n",
      "[8,   1060] loss: 0.006543\n",
      "[8,   1080] loss: 0.006432\n",
      "[8,   1100] loss: 0.006838\n",
      "[8,   1120] loss: 0.007243\n",
      "[8,   1140] loss: 0.006679\n",
      "[8,   1160] loss: 0.007492\n",
      "[8,   1180] loss: 0.006419\n",
      "[8,   1200] loss: 0.006881\n",
      "[8,   1220] loss: 0.006596\n",
      "[8,   1240] loss: 0.006509\n",
      "[8,   1260] loss: 0.006715\n",
      "[8,   1280] loss: 0.006317\n",
      "[8,   1300] loss: 0.006245\n",
      "[8,   1320] loss: 0.006812\n",
      "[8,   1340] loss: 0.007410\n",
      "[8,   1360] loss: 0.006781\n",
      "[8,   1380] loss: 0.007543\n",
      "[8,   1400] loss: 0.007241\n",
      "[8,   1420] loss: 0.007253\n",
      "[8,   1440] loss: 0.006484\n",
      "[8,   1460] loss: 0.006695\n",
      "[8,   1480] loss: 0.006324\n",
      "[8,   1500] loss: 0.006921\n",
      "[8,   1520] loss: 0.006559\n",
      "[8,   1540] loss: 0.006250\n",
      "[8,   1560] loss: 0.006339\n",
      "[8,   1580] loss: 0.006464\n",
      "[8,   1600] loss: 0.007462\n",
      "[8,   1620] loss: 0.006890\n",
      "[8,   1640] loss: 0.006457\n",
      "[8,   1660] loss: 0.006091\n",
      "[8,   1680] loss: 0.006479\n",
      "[8,   1700] loss: 0.006390\n",
      "[8,   1720] loss: 0.006382\n",
      "[8,   1740] loss: 0.006642\n",
      "[8,   1760] loss: 0.006865\n",
      "[8,   1780] loss: 0.007128\n",
      "[8,   1800] loss: 0.006232\n",
      "[8,   1820] loss: 0.006259\n",
      "[8,   1840] loss: 0.006462\n",
      "[8,   1860] loss: 0.006382\n",
      "[8,   1880] loss: 0.006416\n",
      "[8,   1900] loss: 0.006314\n",
      "[8,   1920] loss: 0.006262\n",
      "[8,   1940] loss: 0.006997\n",
      "[8,   1960] loss: 0.006443\n",
      "[8,   1980] loss: 0.006152\n",
      "[8,   2000] loss: 0.006552\n",
      "[8,   2020] loss: 0.006537\n",
      "[8,   2040] loss: 0.005982\n",
      "[8,   2060] loss: 0.006904\n",
      "[8,   2080] loss: 0.006912\n",
      "[8,   2100] loss: 0.006723\n",
      "[8,   2120] loss: 0.006140\n",
      "[8,   2140] loss: 0.006594\n",
      "[8,   2160] loss: 0.006413\n",
      "[8,   2180] loss: 0.006912\n",
      "[8,   2200] loss: 0.006351\n",
      "[8,   2220] loss: 0.006524\n",
      "[8,   2240] loss: 0.006579\n",
      "[8,   2260] loss: 0.007574\n",
      "[8,   2280] loss: 0.006673\n",
      "[8,   2300] loss: 0.006555\n",
      "[8,   2320] loss: 0.007033\n",
      "[8,   2340] loss: 0.006609\n",
      "[8,   2360] loss: 0.006245\n",
      "[8,   2380] loss: 0.006831\n",
      "[8,   2400] loss: 0.007112\n",
      "[8,   2420] loss: 0.006660\n",
      "[8,   2440] loss: 0.006405\n",
      "[8,   2460] loss: 0.006824\n",
      "[8,   2480] loss: 0.006282\n",
      "[8,   2500] loss: 0.006058\n",
      "[9,     20] loss: 0.006174\n",
      "[9,     40] loss: 0.006497\n",
      "[9,     60] loss: 0.006429\n",
      "[9,     80] loss: 0.006237\n",
      "[9,    100] loss: 0.006303\n",
      "[9,    120] loss: 0.006160\n",
      "[9,    140] loss: 0.006379\n",
      "[9,    160] loss: 0.006585\n",
      "[9,    180] loss: 0.005990\n",
      "[9,    200] loss: 0.006228\n",
      "[9,    220] loss: 0.006118\n",
      "[9,    240] loss: 0.006204\n",
      "[9,    260] loss: 0.007140\n",
      "[9,    280] loss: 0.008419\n",
      "[9,    300] loss: 0.007179\n",
      "[9,    320] loss: 0.006721\n",
      "[9,    340] loss: 0.006199\n",
      "[9,    360] loss: 0.006525\n",
      "[9,    380] loss: 0.007754\n",
      "[9,    400] loss: 0.007505\n",
      "[9,    420] loss: 0.006506\n",
      "[9,    440] loss: 0.006411\n",
      "[9,    460] loss: 0.006021\n",
      "[9,    480] loss: 0.006862\n",
      "[9,    500] loss: 0.006915\n",
      "[9,    520] loss: 0.006507\n",
      "[9,    540] loss: 0.006011\n",
      "[9,    560] loss: 0.006211\n",
      "[9,    580] loss: 0.006061\n",
      "[9,    600] loss: 0.006583\n",
      "[9,    620] loss: 0.006549\n",
      "[9,    640] loss: 0.007253\n",
      "[9,    660] loss: 0.006420\n",
      "[9,    680] loss: 0.006461\n",
      "[9,    700] loss: 0.006301\n",
      "[9,    720] loss: 0.006268\n",
      "[9,    740] loss: 0.006319\n",
      "[9,    760] loss: 0.006079\n",
      "[9,    780] loss: 0.006318\n",
      "[9,    800] loss: 0.006994\n",
      "[9,    820] loss: 0.006820\n",
      "[9,    840] loss: 0.006427\n",
      "[9,    860] loss: 0.007169\n",
      "[9,    880] loss: 0.006286\n",
      "[9,    900] loss: 0.005986\n",
      "[9,    920] loss: 0.006074\n",
      "[9,    940] loss: 0.006310\n",
      "[9,    960] loss: 0.006158\n",
      "[9,    980] loss: 0.006004\n",
      "[9,   1000] loss: 0.006924\n",
      "[9,   1020] loss: 0.006199\n",
      "[9,   1040] loss: 0.006556\n",
      "[9,   1060] loss: 0.006396\n",
      "[9,   1080] loss: 0.006684\n",
      "[9,   1100] loss: 0.006557\n",
      "[9,   1120] loss: 0.006531\n",
      "[9,   1140] loss: 0.007287\n",
      "[9,   1160] loss: 0.006917\n",
      "[9,   1180] loss: 0.007256\n",
      "[9,   1200] loss: 0.007226\n",
      "[9,   1220] loss: 0.006879\n",
      "[9,   1240] loss: 0.006203\n",
      "[9,   1260] loss: 0.006639\n",
      "[9,   1280] loss: 0.007430\n",
      "[9,   1300] loss: 0.006667\n",
      "[9,   1320] loss: 0.006491\n",
      "[9,   1340] loss: 0.006764\n",
      "[9,   1360] loss: 0.006860\n",
      "[9,   1380] loss: 0.006007\n",
      "[9,   1400] loss: 0.006278\n",
      "[9,   1420] loss: 0.006202\n",
      "[9,   1440] loss: 0.006352\n",
      "[9,   1460] loss: 0.005956\n",
      "[9,   1480] loss: 0.006127\n",
      "[9,   1500] loss: 0.006373\n",
      "[9,   1520] loss: 0.006573\n",
      "[9,   1540] loss: 0.007355\n",
      "[9,   1560] loss: 0.006456\n",
      "[9,   1580] loss: 0.006384\n",
      "[9,   1600] loss: 0.006448\n",
      "[9,   1620] loss: 0.008314\n",
      "[9,   1640] loss: 0.007112\n",
      "[9,   1660] loss: 0.006834\n",
      "[9,   1680] loss: 0.006560\n",
      "[9,   1700] loss: 0.006760\n",
      "[9,   1720] loss: 0.006834\n",
      "[9,   1740] loss: 0.006189\n",
      "[9,   1760] loss: 0.006294\n",
      "[9,   1780] loss: 0.006542\n",
      "[9,   1800] loss: 0.006120\n",
      "[9,   1820] loss: 0.006319\n",
      "[9,   1840] loss: 0.006549\n",
      "[9,   1860] loss: 0.005986\n",
      "[9,   1880] loss: 0.006084\n",
      "[9,   1900] loss: 0.006253\n",
      "[9,   1920] loss: 0.006384\n",
      "[9,   1940] loss: 0.006841\n",
      "[9,   1960] loss: 0.006220\n",
      "[9,   1980] loss: 0.006292\n",
      "[9,   2000] loss: 0.006359\n",
      "[9,   2020] loss: 0.006127\n",
      "[9,   2040] loss: 0.006175\n",
      "[9,   2060] loss: 0.005977\n",
      "[9,   2080] loss: 0.006372\n",
      "[9,   2100] loss: 0.006611\n",
      "[9,   2120] loss: 0.006146\n",
      "[9,   2140] loss: 0.006062\n",
      "[9,   2160] loss: 0.006125\n",
      "[9,   2180] loss: 0.006127\n",
      "[9,   2200] loss: 0.005993\n",
      "[9,   2220] loss: 0.006423\n",
      "[9,   2240] loss: 0.006577\n",
      "[9,   2260] loss: 0.006314\n",
      "[9,   2280] loss: 0.006054\n",
      "[9,   2300] loss: 0.005720\n",
      "[9,   2320] loss: 0.006223\n",
      "[9,   2340] loss: 0.006171\n",
      "[9,   2360] loss: 0.006627\n",
      "[9,   2380] loss: 0.006194\n",
      "[9,   2400] loss: 0.005914\n",
      "[9,   2420] loss: 0.006352\n",
      "[9,   2440] loss: 0.006249\n",
      "[9,   2460] loss: 0.006127\n",
      "[9,   2480] loss: 0.006768\n",
      "[9,   2500] loss: 0.006102\n",
      "[10,     20] loss: 0.006417\n",
      "[10,     40] loss: 0.006750\n",
      "[10,     60] loss: 0.006302\n",
      "[10,     80] loss: 0.006097\n",
      "[10,    100] loss: 0.005901\n",
      "[10,    120] loss: 0.006560\n",
      "[10,    140] loss: 0.006887\n",
      "[10,    160] loss: 0.006567\n",
      "[10,    180] loss: 0.006082\n",
      "[10,    200] loss: 0.005943\n",
      "[10,    220] loss: 0.006236\n",
      "[10,    240] loss: 0.005892\n",
      "[10,    260] loss: 0.006295\n",
      "[10,    280] loss: 0.006172\n",
      "[10,    300] loss: 0.006330\n",
      "[10,    320] loss: 0.006850\n",
      "[10,    340] loss: 0.006091\n",
      "[10,    360] loss: 0.007824\n",
      "[10,    380] loss: 0.006955\n",
      "[10,    400] loss: 0.006684\n",
      "[10,    420] loss: 0.006522\n",
      "[10,    440] loss: 0.006708\n",
      "[10,    460] loss: 0.006653\n",
      "[10,    480] loss: 0.006212\n",
      "[10,    500] loss: 0.006276\n",
      "[10,    520] loss: 0.006652\n",
      "[10,    540] loss: 0.006429\n",
      "[10,    560] loss: 0.006060\n",
      "[10,    580] loss: 0.005907\n",
      "[10,    600] loss: 0.006166\n",
      "[10,    620] loss: 0.006133\n",
      "[10,    640] loss: 0.007092\n",
      "[10,    660] loss: 0.006765\n",
      "[10,    680] loss: 0.006099\n",
      "[10,    700] loss: 0.006207\n",
      "[10,    720] loss: 0.006087\n",
      "[10,    740] loss: 0.005734\n",
      "[10,    760] loss: 0.006134\n",
      "[10,    780] loss: 0.006997\n",
      "[10,    800] loss: 0.006151\n",
      "[10,    820] loss: 0.005973\n",
      "[10,    840] loss: 0.006128\n",
      "[10,    860] loss: 0.006298\n",
      "[10,    880] loss: 0.006913\n",
      "[10,    900] loss: 0.006249\n",
      "[10,    920] loss: 0.006787\n",
      "[10,    940] loss: 0.006353\n",
      "[10,    960] loss: 0.006044\n",
      "[10,    980] loss: 0.005722\n",
      "[10,   1000] loss: 0.007140\n",
      "[10,   1020] loss: 0.006727\n",
      "[10,   1040] loss: 0.006599\n",
      "[10,   1060] loss: 0.006593\n",
      "[10,   1080] loss: 0.005827\n",
      "[10,   1100] loss: 0.005992\n",
      "[10,   1120] loss: 0.007477\n",
      "[10,   1140] loss: 0.006577\n",
      "[10,   1160] loss: 0.006186\n",
      "[10,   1180] loss: 0.006367\n",
      "[10,   1200] loss: 0.006772\n",
      "[10,   1220] loss: 0.006941\n",
      "[10,   1240] loss: 0.007107\n",
      "[10,   1260] loss: 0.007117\n",
      "[10,   1280] loss: 0.006187\n",
      "[10,   1300] loss: 0.005970\n",
      "[10,   1320] loss: 0.006254\n",
      "[10,   1340] loss: 0.006067\n",
      "[10,   1360] loss: 0.006517\n",
      "[10,   1380] loss: 0.006510\n",
      "[10,   1400] loss: 0.006517\n",
      "[10,   1420] loss: 0.006025\n",
      "[10,   1440] loss: 0.006049\n",
      "[10,   1460] loss: 0.006097\n",
      "[10,   1480] loss: 0.006070\n",
      "[10,   1500] loss: 0.006073\n",
      "[10,   1520] loss: 0.006160\n",
      "[10,   1540] loss: 0.007046\n",
      "[10,   1560] loss: 0.006770\n",
      "[10,   1580] loss: 0.006237\n",
      "[10,   1600] loss: 0.006267\n",
      "[10,   1620] loss: 0.006036\n",
      "[10,   1640] loss: 0.006136\n",
      "[10,   1660] loss: 0.005904\n",
      "[10,   1680] loss: 0.006695\n",
      "[10,   1700] loss: 0.006472\n",
      "[10,   1720] loss: 0.006894\n",
      "[10,   1740] loss: 0.006232\n",
      "[10,   1760] loss: 0.007104\n",
      "[10,   1780] loss: 0.006305\n",
      "[10,   1800] loss: 0.006278\n",
      "[10,   1820] loss: 0.006625\n",
      "[10,   1840] loss: 0.006347\n",
      "[10,   1860] loss: 0.006319\n",
      "[10,   1880] loss: 0.006158\n",
      "[10,   1900] loss: 0.005973\n",
      "[10,   1920] loss: 0.006416\n",
      "[10,   1940] loss: 0.006303\n",
      "[10,   1960] loss: 0.006137\n",
      "[10,   1980] loss: 0.005781\n",
      "[10,   2000] loss: 0.005762\n",
      "[10,   2020] loss: 0.006184\n",
      "[10,   2040] loss: 0.006555\n",
      "[10,   2060] loss: 0.006569\n",
      "[10,   2080] loss: 0.006648\n",
      "[10,   2100] loss: 0.006148\n",
      "[10,   2120] loss: 0.006225\n",
      "[10,   2140] loss: 0.006205\n",
      "[10,   2160] loss: 0.006097\n",
      "[10,   2180] loss: 0.006341\n",
      "[10,   2200] loss: 0.006392\n",
      "[10,   2220] loss: 0.006639\n",
      "[10,   2240] loss: 0.006069\n",
      "[10,   2260] loss: 0.006266\n",
      "[10,   2280] loss: 0.006420\n",
      "[10,   2300] loss: 0.006175\n",
      "[10,   2320] loss: 0.006120\n",
      "[10,   2340] loss: 0.005798\n",
      "[10,   2360] loss: 0.005838\n",
      "[10,   2380] loss: 0.006080\n",
      "[10,   2400] loss: 0.006365\n",
      "[10,   2420] loss: 0.006860\n",
      "[10,   2440] loss: 0.006390\n",
      "[10,   2460] loss: 0.007062\n",
      "[10,   2480] loss: 0.010000\n",
      "[10,   2500] loss: 0.008094\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(config.epochs):  # loop over the dataset multiple times\n",
    "\n",
    "  running_loss = 0.0\n",
    "\n",
    "  for i, (img, labels) in enumerate(dataloader):\n",
    "        \n",
    "    img = torch.as_tensor(img, dtype=torch.float32).cuda()\n",
    "\n",
    "    labels = torch.as_tensor(labels, dtype=torch.float32).cuda()    \n",
    "    labels = labels.flatten()\n",
    "    labels = torch.reshape(labels, (config.batch_size, 4*config.objects))\n",
    "     \n",
    "    optimizer.zero_grad()\n",
    "    out = model(img)\n",
    "        \n",
    "   \n",
    "    loss = criterion(out, labels)             \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # print statistics\n",
    "    running_loss += loss.item()\n",
    "    if i % 20 == 19:    # print every 2000 mini-batches\n",
    "        wandb.log({'loss': running_loss / 20})\n",
    "        print('[%d, %6d] loss: %.6f' %\n",
    "              (epoch + 1, i + 1, running_loss / 20))\n",
    "      \n",
    "        running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input image:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<svg height=\"250.618594pt\" version=\"1.1\" viewBox=\"0 0 251.565 250.618594\" width=\"251.565pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-06-19T01:12:32.047769</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.4.2, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 250.618594 \r\nL 251.565 250.618594 \r\nL 251.565 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 26.925 226.740469 \r\nL 244.365 226.740469 \r\nL 244.365 9.300469 \r\nL 26.925 9.300469 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g clip-path=\"url(#p35c57655e5)\">\r\n    <image height=\"218\" id=\"imageb2e6c8bb44\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"26.925\" xlink:href=\"data:image/png;base64,\r\niVBORw0KGgoAAAANSUhEUgAAANoAAADaCAYAAADAHVzbAAACkUlEQVR4nO3VsQ2DUBQEwYflhLpct+sisQRNoP0SnqngktVtM3MOt/oev9UTbvPZ36snPMJr9QD4B0KDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQLbzJyrR8DTeTQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoPABYUNBrGSzWq+AAAAAElFTkSuQmCC\" y=\"-8.740469\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m4a5c1095d3\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"28.62375\" xlink:href=\"#m4a5c1095d3\" y=\"226.740469\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(25.4425 241.338906)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2034 4250 \r\nQ 1547 4250 1301 3770 \r\nQ 1056 3291 1056 2328 \r\nQ 1056 1369 1301 889 \r\nQ 1547 409 2034 409 \r\nQ 2525 409 2770 889 \r\nQ 3016 1369 3016 2328 \r\nQ 3016 3291 2770 3770 \r\nQ 2525 4250 2034 4250 \r\nz\r\nM 2034 4750 \r\nQ 2819 4750 3233 4129 \r\nQ 3647 3509 3647 2328 \r\nQ 3647 1150 3233 529 \r\nQ 2819 -91 2034 -91 \r\nQ 1250 -91 836 529 \r\nQ 422 1150 422 2328 \r\nQ 422 3509 836 4129 \r\nQ 1250 4750 2034 4750 \r\nz\r\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"62.59875\" xlink:href=\"#m4a5c1095d3\" y=\"226.740469\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(56.23625 241.338906)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 794 531 \r\nL 1825 531 \r\nL 1825 4091 \r\nL 703 3866 \r\nL 703 4441 \r\nL 1819 4666 \r\nL 2450 4666 \r\nL 2450 531 \r\nL 3481 531 \r\nL 3481 0 \r\nL 794 0 \r\nL 794 531 \r\nz\r\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"96.57375\" xlink:href=\"#m4a5c1095d3\" y=\"226.740469\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(90.21125 241.338906)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 1228 531 \r\nL 3431 531 \r\nL 3431 0 \r\nL 469 0 \r\nL 469 531 \r\nQ 828 903 1448 1529 \r\nQ 2069 2156 2228 2338 \r\nQ 2531 2678 2651 2914 \r\nQ 2772 3150 2772 3378 \r\nQ 2772 3750 2511 3984 \r\nQ 2250 4219 1831 4219 \r\nQ 1534 4219 1204 4116 \r\nQ 875 4013 500 3803 \r\nL 500 4441 \r\nQ 881 4594 1212 4672 \r\nQ 1544 4750 1819 4750 \r\nQ 2544 4750 2975 4387 \r\nQ 3406 4025 3406 3419 \r\nQ 3406 3131 3298 2873 \r\nQ 3191 2616 2906 2266 \r\nQ 2828 2175 2409 1742 \r\nQ 1991 1309 1228 531 \r\nz\r\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"130.54875\" xlink:href=\"#m4a5c1095d3\" y=\"226.740469\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 30 -->\r\n      <g transform=\"translate(124.18625 241.338906)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2597 2516 \r\nQ 3050 2419 3304 2112 \r\nQ 3559 1806 3559 1356 \r\nQ 3559 666 3084 287 \r\nQ 2609 -91 1734 -91 \r\nQ 1441 -91 1130 -33 \r\nQ 819 25 488 141 \r\nL 488 750 \r\nQ 750 597 1062 519 \r\nQ 1375 441 1716 441 \r\nQ 2309 441 2620 675 \r\nQ 2931 909 2931 1356 \r\nQ 2931 1769 2642 2001 \r\nQ 2353 2234 1838 2234 \r\nL 1294 2234 \r\nL 1294 2753 \r\nL 1863 2753 \r\nQ 2328 2753 2575 2939 \r\nQ 2822 3125 2822 3475 \r\nQ 2822 3834 2567 4026 \r\nQ 2313 4219 1838 4219 \r\nQ 1578 4219 1281 4162 \r\nQ 984 4106 628 3988 \r\nL 628 4550 \r\nQ 988 4650 1302 4700 \r\nQ 1616 4750 1894 4750 \r\nQ 2613 4750 3031 4423 \r\nQ 3450 4097 3450 3541 \r\nQ 3450 3153 3228 2886 \r\nQ 3006 2619 2597 2516 \r\nz\r\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-33\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"164.52375\" xlink:href=\"#m4a5c1095d3\" y=\"226.740469\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 40 -->\r\n      <g transform=\"translate(158.16125 241.338906)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2419 4116 \r\nL 825 1625 \r\nL 2419 1625 \r\nL 2419 4116 \r\nz\r\nM 2253 4666 \r\nL 3047 4666 \r\nL 3047 1625 \r\nL 3713 1625 \r\nL 3713 1100 \r\nL 3047 1100 \r\nL 3047 0 \r\nL 2419 0 \r\nL 2419 1100 \r\nL 313 1100 \r\nL 313 1709 \r\nL 2253 4666 \r\nz\r\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-34\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"198.49875\" xlink:href=\"#m4a5c1095d3\" y=\"226.740469\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 50 -->\r\n      <g transform=\"translate(192.13625 241.338906)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 691 4666 \r\nL 3169 4666 \r\nL 3169 4134 \r\nL 1269 4134 \r\nL 1269 2991 \r\nQ 1406 3038 1543 3061 \r\nQ 1681 3084 1819 3084 \r\nQ 2600 3084 3056 2656 \r\nQ 3513 2228 3513 1497 \r\nQ 3513 744 3044 326 \r\nQ 2575 -91 1722 -91 \r\nQ 1428 -91 1123 -41 \r\nQ 819 9 494 109 \r\nL 494 744 \r\nQ 775 591 1075 516 \r\nQ 1375 441 1709 441 \r\nQ 2250 441 2565 725 \r\nQ 2881 1009 2881 1497 \r\nQ 2881 1984 2565 2268 \r\nQ 2250 2553 1709 2553 \r\nQ 1456 2553 1204 2497 \r\nQ 953 2441 691 2322 \r\nL 691 4666 \r\nz\r\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-35\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"232.47375\" xlink:href=\"#m4a5c1095d3\" y=\"226.740469\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 60 -->\r\n      <g transform=\"translate(226.11125 241.338906)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2113 2584 \r\nQ 1688 2584 1439 2293 \r\nQ 1191 2003 1191 1497 \r\nQ 1191 994 1439 701 \r\nQ 1688 409 2113 409 \r\nQ 2538 409 2786 701 \r\nQ 3034 994 3034 1497 \r\nQ 3034 2003 2786 2293 \r\nQ 2538 2584 2113 2584 \r\nz\r\nM 3366 4563 \r\nL 3366 3988 \r\nQ 3128 4100 2886 4159 \r\nQ 2644 4219 2406 4219 \r\nQ 1781 4219 1451 3797 \r\nQ 1122 3375 1075 2522 \r\nQ 1259 2794 1537 2939 \r\nQ 1816 3084 2150 3084 \r\nQ 2853 3084 3261 2657 \r\nQ 3669 2231 3669 1497 \r\nQ 3669 778 3244 343 \r\nQ 2819 -91 2113 -91 \r\nQ 1303 -91 875 529 \r\nQ 447 1150 447 2328 \r\nQ 447 3434 972 4092 \r\nQ 1497 4750 2381 4750 \r\nQ 2619 4750 2861 4703 \r\nQ 3103 4656 3366 4563 \r\nz\r\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-36\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_8\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m3d0a74146c\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m3d0a74146c\" y=\"10.999219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(13.5625 14.798437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m3d0a74146c\" y=\"44.974219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(7.2 48.773437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m3d0a74146c\" y=\"78.949219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(7.2 82.748437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m3d0a74146c\" y=\"112.924219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 30 -->\r\n      <g transform=\"translate(7.2 116.723437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-33\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m3d0a74146c\" y=\"146.899219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 40 -->\r\n      <g transform=\"translate(7.2 150.698437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-34\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m3d0a74146c\" y=\"180.874219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 50 -->\r\n      <g transform=\"translate(7.2 184.673437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-35\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m3d0a74146c\" y=\"214.849219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 60 -->\r\n      <g transform=\"translate(7.2 218.648437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-36\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 26.925 226.740469 \r\nL 26.925 9.300469 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 244.365 226.740469 \r\nL 244.365 9.300469 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 26.925 226.740469 \r\nL 244.365 226.740469 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 26.925 9.300469 \r\nL 244.365 9.300469 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p35c57655e5\">\r\n   <rect height=\"217.44\" width=\"217.44\" x=\"26.925\" y=\"9.300469\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input box location (x, y), (w, h):\n(25, 40) (39, 86)\n--------------------------------------------------\npredicted output:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<svg height=\"250.618594pt\" version=\"1.1\" viewBox=\"0 0 251.565 250.618594\" width=\"251.565pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-06-19T01:12:32.170769</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.4.2, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 250.618594 \r\nL 251.565 250.618594 \r\nL 251.565 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 26.925 226.740469 \r\nL 244.365 226.740469 \r\nL 244.365 9.300469 \r\nL 26.925 9.300469 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g clip-path=\"url(#p4c523f15fe)\">\r\n    <image height=\"218\" id=\"image09338df369\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"26.925\" xlink:href=\"data:image/png;base64,\r\niVBORw0KGgoAAAANSUhEUgAAANoAAADaCAYAAADAHVzbAAACkUlEQVR4nO3VsQ3CUBQEQT9LJNRF2dTlxIFpAvYLmKngktXNzFwbb/U8ztUTPuZxv62e8JX21QPgHwgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQIzMxcq0fAr/NoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAaBF1ZLBrdJx/4OAAAAAElFTkSuQmCC\" y=\"-8.740469\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m6aa1345616\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"28.62375\" xlink:href=\"#m6aa1345616\" y=\"226.740469\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(25.4425 241.338906)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2034 4250 \r\nQ 1547 4250 1301 3770 \r\nQ 1056 3291 1056 2328 \r\nQ 1056 1369 1301 889 \r\nQ 1547 409 2034 409 \r\nQ 2525 409 2770 889 \r\nQ 3016 1369 3016 2328 \r\nQ 3016 3291 2770 3770 \r\nQ 2525 4250 2034 4250 \r\nz\r\nM 2034 4750 \r\nQ 2819 4750 3233 4129 \r\nQ 3647 3509 3647 2328 \r\nQ 3647 1150 3233 529 \r\nQ 2819 -91 2034 -91 \r\nQ 1250 -91 836 529 \r\nQ 422 1150 422 2328 \r\nQ 422 3509 836 4129 \r\nQ 1250 4750 2034 4750 \r\nz\r\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"62.59875\" xlink:href=\"#m6aa1345616\" y=\"226.740469\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(56.23625 241.338906)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 794 531 \r\nL 1825 531 \r\nL 1825 4091 \r\nL 703 3866 \r\nL 703 4441 \r\nL 1819 4666 \r\nL 2450 4666 \r\nL 2450 531 \r\nL 3481 531 \r\nL 3481 0 \r\nL 794 0 \r\nL 794 531 \r\nz\r\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"96.57375\" xlink:href=\"#m6aa1345616\" y=\"226.740469\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(90.21125 241.338906)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 1228 531 \r\nL 3431 531 \r\nL 3431 0 \r\nL 469 0 \r\nL 469 531 \r\nQ 828 903 1448 1529 \r\nQ 2069 2156 2228 2338 \r\nQ 2531 2678 2651 2914 \r\nQ 2772 3150 2772 3378 \r\nQ 2772 3750 2511 3984 \r\nQ 2250 4219 1831 4219 \r\nQ 1534 4219 1204 4116 \r\nQ 875 4013 500 3803 \r\nL 500 4441 \r\nQ 881 4594 1212 4672 \r\nQ 1544 4750 1819 4750 \r\nQ 2544 4750 2975 4387 \r\nQ 3406 4025 3406 3419 \r\nQ 3406 3131 3298 2873 \r\nQ 3191 2616 2906 2266 \r\nQ 2828 2175 2409 1742 \r\nQ 1991 1309 1228 531 \r\nz\r\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"130.54875\" xlink:href=\"#m6aa1345616\" y=\"226.740469\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 30 -->\r\n      <g transform=\"translate(124.18625 241.338906)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2597 2516 \r\nQ 3050 2419 3304 2112 \r\nQ 3559 1806 3559 1356 \r\nQ 3559 666 3084 287 \r\nQ 2609 -91 1734 -91 \r\nQ 1441 -91 1130 -33 \r\nQ 819 25 488 141 \r\nL 488 750 \r\nQ 750 597 1062 519 \r\nQ 1375 441 1716 441 \r\nQ 2309 441 2620 675 \r\nQ 2931 909 2931 1356 \r\nQ 2931 1769 2642 2001 \r\nQ 2353 2234 1838 2234 \r\nL 1294 2234 \r\nL 1294 2753 \r\nL 1863 2753 \r\nQ 2328 2753 2575 2939 \r\nQ 2822 3125 2822 3475 \r\nQ 2822 3834 2567 4026 \r\nQ 2313 4219 1838 4219 \r\nQ 1578 4219 1281 4162 \r\nQ 984 4106 628 3988 \r\nL 628 4550 \r\nQ 988 4650 1302 4700 \r\nQ 1616 4750 1894 4750 \r\nQ 2613 4750 3031 4423 \r\nQ 3450 4097 3450 3541 \r\nQ 3450 3153 3228 2886 \r\nQ 3006 2619 2597 2516 \r\nz\r\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-33\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"164.52375\" xlink:href=\"#m6aa1345616\" y=\"226.740469\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 40 -->\r\n      <g transform=\"translate(158.16125 241.338906)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2419 4116 \r\nL 825 1625 \r\nL 2419 1625 \r\nL 2419 4116 \r\nz\r\nM 2253 4666 \r\nL 3047 4666 \r\nL 3047 1625 \r\nL 3713 1625 \r\nL 3713 1100 \r\nL 3047 1100 \r\nL 3047 0 \r\nL 2419 0 \r\nL 2419 1100 \r\nL 313 1100 \r\nL 313 1709 \r\nL 2253 4666 \r\nz\r\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-34\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"198.49875\" xlink:href=\"#m6aa1345616\" y=\"226.740469\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 50 -->\r\n      <g transform=\"translate(192.13625 241.338906)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 691 4666 \r\nL 3169 4666 \r\nL 3169 4134 \r\nL 1269 4134 \r\nL 1269 2991 \r\nQ 1406 3038 1543 3061 \r\nQ 1681 3084 1819 3084 \r\nQ 2600 3084 3056 2656 \r\nQ 3513 2228 3513 1497 \r\nQ 3513 744 3044 326 \r\nQ 2575 -91 1722 -91 \r\nQ 1428 -91 1123 -41 \r\nQ 819 9 494 109 \r\nL 494 744 \r\nQ 775 591 1075 516 \r\nQ 1375 441 1709 441 \r\nQ 2250 441 2565 725 \r\nQ 2881 1009 2881 1497 \r\nQ 2881 1984 2565 2268 \r\nQ 2250 2553 1709 2553 \r\nQ 1456 2553 1204 2497 \r\nQ 953 2441 691 2322 \r\nL 691 4666 \r\nz\r\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-35\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"232.47375\" xlink:href=\"#m6aa1345616\" y=\"226.740469\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 60 -->\r\n      <g transform=\"translate(226.11125 241.338906)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2113 2584 \r\nQ 1688 2584 1439 2293 \r\nQ 1191 2003 1191 1497 \r\nQ 1191 994 1439 701 \r\nQ 1688 409 2113 409 \r\nQ 2538 409 2786 701 \r\nQ 3034 994 3034 1497 \r\nQ 3034 2003 2786 2293 \r\nQ 2538 2584 2113 2584 \r\nz\r\nM 3366 4563 \r\nL 3366 3988 \r\nQ 3128 4100 2886 4159 \r\nQ 2644 4219 2406 4219 \r\nQ 1781 4219 1451 3797 \r\nQ 1122 3375 1075 2522 \r\nQ 1259 2794 1537 2939 \r\nQ 1816 3084 2150 3084 \r\nQ 2853 3084 3261 2657 \r\nQ 3669 2231 3669 1497 \r\nQ 3669 778 3244 343 \r\nQ 2819 -91 2113 -91 \r\nQ 1303 -91 875 529 \r\nQ 447 1150 447 2328 \r\nQ 447 3434 972 4092 \r\nQ 1497 4750 2381 4750 \r\nQ 2619 4750 2861 4703 \r\nQ 3103 4656 3366 4563 \r\nz\r\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-36\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_8\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m9303ddc6e7\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m9303ddc6e7\" y=\"10.999219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(13.5625 14.798437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m9303ddc6e7\" y=\"44.974219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(7.2 48.773437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m9303ddc6e7\" y=\"78.949219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(7.2 82.748437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m9303ddc6e7\" y=\"112.924219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 30 -->\r\n      <g transform=\"translate(7.2 116.723437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-33\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m9303ddc6e7\" y=\"146.899219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 40 -->\r\n      <g transform=\"translate(7.2 150.698437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-34\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m9303ddc6e7\" y=\"180.874219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 50 -->\r\n      <g transform=\"translate(7.2 184.673437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-35\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m9303ddc6e7\" y=\"214.849219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 60 -->\r\n      <g transform=\"translate(7.2 218.648437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-36\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 26.925 226.740469 \r\nL 26.925 9.300469 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 244.365 226.740469 \r\nL 244.365 9.300469 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 26.925 226.740469 \r\nL 244.365 226.740469 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 26.925 9.300469 \r\nL 244.365 9.300469 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p4c523f15fe\">\r\n   <rect height=\"217.44\" width=\"217.44\" x=\"26.925\" y=\"9.300469\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "predicted box location (x, y), (w, h):\n(25, 41) (40, 72)\n--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# testing the trained model\n",
    "\n",
    "model = model.eval()\n",
    "\n",
    "np.random.seed(np.random.randint(0, 99999))\n",
    "\n",
    "image = np.zeros((config.size,config.size, 3), np.uint8)\n",
    "labels = []\n",
    "model = model.cuda()\n",
    "\n",
    "point_x = int(np.random.rand() * config.size)\n",
    "point_y = int(np.random.rand() * config.size)\n",
    "size_x = int(np.random.rand() * config.size)\n",
    "size_y = int(np.random.rand() * config.size)       \n",
    "r, g, b = int(np.random.rand()*255), int(np.random.rand()*255), int(np.random.rand() * 255)  \n",
    "\n",
    "cv2.rectangle(image, (int(point_x - size_x/2), int(point_y - size_y/2)), (int(point_x + size_x/2), int(point_y + size_y/2)), (r, g, b), -1)            \n",
    "\n",
    "print(\"input image:\")\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "print(\"input box location (x, y), (w, h):\")\n",
    "print((int(point_x - size_x/2), int(point_y - size_y/2)), (int(point_x + size_x/2), int(point_y + size_y/2)))\n",
    "\n",
    "image_tensor = torch.as_tensor(image, dtype=torch.float32)\n",
    "\n",
    "image_tensor = torch.unsqueeze(image_tensor, 0).cuda()    \n",
    "\n",
    "image2 = np.ones((config.size,config.size, 3), np.uint8)\n",
    "\n",
    "t1 = time.time()\n",
    "for i in range(1):\n",
    "  out = model(image_tensor)\n",
    "  out = out[0].detach().cpu().numpy()\n",
    "  \n",
    "out_point_x = int(out[0] * config.size)\n",
    "out_point_y = int(out[1] * config.size)\n",
    "out_size_x = int(out[2] * config.size)\n",
    "out_size_y = int(out[3] * config.size)\n",
    "\n",
    "cv2.rectangle(image2, (int(out_point_x - out_size_x/2), int(out_point_y - out_size_y/2)), (int(out_point_x + out_size_x/2), int(out_point_y + out_size_y/2)), (r, g, b), -1)            \n",
    "\n",
    "print(\"-\"*50)\n",
    "print(\"predicted output:\")\n",
    "plt.imshow(image2)\n",
    "plt.show()\n",
    "print(\"predicted box location (x, y), (w, h):\")\n",
    "print((int(out_point_x - out_size_x/2), int(out_point_y - out_size_y/2)), (int(out_point_x + out_size_x/2), int(out_point_y + out_size_y/2)))\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}